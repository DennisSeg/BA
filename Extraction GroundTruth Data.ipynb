{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook shows how the geo-tagged tweets were extracted and how the test and training\n",
    "# files for the models were created\n",
    "\n",
    "# First we will extract users with geo-tagged tweets and analyse them\n",
    "import json\n",
    "import csv\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "\n",
    "# Path to hydrated TweetsCOV19\n",
    "pathHydrated = \"C:/Users/dennis/Desktop/hydrated.jsonl\"\n",
    "\n",
    "data ={}\n",
    "dUserCoordinates ={}\n",
    "countGeoTag=0\n",
    "with open(pathHydrated,encoding=\"utf8\",errors='ignore') as hydratedJSONL:\n",
    "    count = 0\n",
    "    for raw in hydratedJSONL:\n",
    "        tweet = json.loads(raw)\n",
    "        # We will consider the id_str of the use, instead one can take the username feature \"screen_name\"\n",
    "        user = tweet[\"user\"][\"id_str\"]\n",
    "       \n",
    "        #We will prioritize the \"geo\" feature\n",
    "        geo = tweet[\"geo\"]\n",
    "        if geo != None:\n",
    "            coordinates = tweet[\"geo\"][\"coordinates\"]\n",
    "            countGeoTag+=1\n",
    "\n",
    "            # Created dict with user: (coordinates)\n",
    "            if user not in dUserCoordinates.keys():\n",
    "                dUserCoordinates[user]=[]\n",
    "                dUserCoordinates[user].append(tuple(coordinates))\n",
    "            else:\n",
    "                dUserCoordinates[user].append(tuple(coordinates))\n",
    "                       \n",
    "            #Count Number GeoTagged Tweets per User\n",
    "            if user not in data.keys():\n",
    "                data[user]=1                \n",
    "                continue\n",
    "            else:\n",
    "                data[user]+=1\n",
    "                continue\n",
    "                \n",
    "        # Else we will consider the \"place\" feauture       \n",
    "        if (tweet[\"place\"]!=None) and geo == None  :\n",
    "            if tweet[\"place\"][\"bounding_box\"] != None:\n",
    "                countGeoTag+=1\n",
    "                #Coordinates have to be swapped from long,lat -> lat,long\n",
    "                coordinates = list(tweet[\"place\"][\"bounding_box\"][\"coordinates\"][0][0])\n",
    "                coordinates = (coordinates[1],coordinates[0])\n",
    "                \n",
    "                # Created dict with user: (coordinates)\n",
    "                if user not in dUserCoordinates.keys():\n",
    "                    dUserCoordinates[user]=[]\n",
    "                    dUserCoordinates[user].append(tuple(coordinates))\n",
    "                else:\n",
    "                    dUserCoordinates[user].append(tuple(coordinates))\n",
    "                \n",
    "                #Count Number GeoTagged Tweets per User\n",
    "                if user not in data.keys():\n",
    "                    data[user]=1\n",
    "                    continue\n",
    "                else:\n",
    "                    data[user]+=1\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyse how many users we have with multiple tweets and the breakdown regarding their number of tweets\n",
    "\n",
    "import math\n",
    "\n",
    "count=0\n",
    "maxValue =0\n",
    "for x,value in data.items():\n",
    "    if value > maxValue:\n",
    "        maxValue = value\n",
    "    if value >1:\n",
    "        count +=1\n",
    "\n",
    "gt300=0\n",
    "gt100=0\n",
    "gt50=0\n",
    "gt10=0\n",
    "gt5=0\n",
    "gt1=0\n",
    "\n",
    "for y in data.values():\n",
    "\n",
    "    if y>300:\n",
    "        gt300=gt300+1\n",
    "    elif y>100:\n",
    "        gt100=gt100+1\n",
    "    elif y>50:\n",
    "        gt50=gt50+1\n",
    "    elif y>10:\n",
    "        gt10=gt10+1\n",
    "    elif y>5:\n",
    "        gt5=gt5+1\n",
    "    elif y>1:\n",
    "        gt1=gt1+1\n",
    "    elif y==1:\n",
    "        eq1+=1\n",
    "\n",
    "\n",
    "print(\"GeoTagged Tweets\")\n",
    "print(\"Number of Tweets : \"+str(countGeoTag))\n",
    "print(\"Number of Users : \"+str(len(data)))\n",
    "print(\"Number of Users with multiple  Tweets: \"+str(count))\n",
    "print(\"Max count of Tweets for a single User: \"+str(maxValue)+\"\\n\")\n",
    "\n",
    "print(\"Distribution of User\")\n",
    "print(\"User     >    Tweets\")\n",
    "print(str(gt300) +\" > 300\")\n",
    "print(str(gt100) +\" > 100\")\n",
    "print(str(gt50) +\" > 50\")\n",
    "print(str(gt10) +\" > 10\")\n",
    "print(str(gt5) +\" > 5\")\n",
    "print(str(gt1) +\" > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to calculate most frequent city for ground truth data ( mobile users)\n",
    "def most_frequent(listCity): \n",
    "    counter = 0\n",
    "    freqCity = listCity[0] \n",
    "    index = 0\n",
    "    count = 0\n",
    "    for i in listCity:\n",
    "        curr_frequency = listCity.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            freqCity = i\n",
    "            index = count\n",
    "        count+=1  \n",
    "        \n",
    "    return freqCity,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Cell shows how ground truth was selected for users.\n",
    "\n",
    "from geopy.distance import distance\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "userLabel={}\n",
    "countUserDifCoord=0\n",
    "dMultiCountry ={}\n",
    "dMultiTweet={}\n",
    "length = len(dUserCoordinates.keys())\n",
    "\n",
    "# Groundtruth for user with single tweets or tweets with equal coordinates\n",
    "for user,value in dUserCoordinates.items():\n",
    "    if len(value)==1 or all(elem == value[0] for elem in value):\n",
    "        userLabel[user]=value[0]\n",
    "                \n",
    "# Groundtruth data for users with different multiple tweet coordinates\n",
    "dMultiTweet=[]\n",
    "for user,value in dUserCoordinates.items():\n",
    "    if len(value)>1 and not all(elem == value[0] for elem in value):\n",
    "        distances=[]\n",
    "        \n",
    "        if count%100==0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "        \n",
    "        #Primary Location is coordinate from the city where the majority of tweets were from     \n",
    "        #Reverse geoCoding\n",
    "        results = rg.search(value)\n",
    "        cities={}\n",
    "        \n",
    "        for x in range(len(value)):\n",
    "            location=results[x]\n",
    "            city=location[\"name\"]\n",
    "            cities[value[x]]=city\n",
    "        \n",
    "        #Get most frequent city and index \n",
    "        fixCity,index = most_frequent(list(cities.values()))\n",
    "        userLabel[user]=value[index]\n",
    "\n",
    "        #Continue if all geocoordinates are the same city \n",
    "        if all(elem == fixCity for x,elem in cities.items()):\n",
    "            userLabel[user]=value[0]\n",
    "            continue\n",
    "        \n",
    "        #Calculate distance if cities are not equal\n",
    "        for x,y in cities.items():\n",
    "            if y == fixCity:\n",
    "                continue\n",
    "            distances.append(round(distance(x,value[index]).km))\n",
    "        \n",
    "        #Calculate avgDistance \n",
    "        avgDistance = int(sum(distances)/len(distances))\n",
    "        dMultiTweet.append(avgDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell analysis mobile users\n",
    "\n",
    "lDistances = sorted(dMultiTweet)\n",
    "length=len(lDistances)\n",
    "index=int(length/2)\n",
    "\n",
    "print(\"Number of Distances  is \"+str(len(lDistances)))\n",
    "avg  = int ( sum(lDistances)/len(lDistances))\n",
    "print(\"Average is \"+str(avg)+\"km\")\n",
    "median = lDistances[index]\n",
    "print(\"Median is \"+str(median)+\"km\")\n",
    "mostFreqDistance, counter = most_frequent(lDistances)\n",
    "print(\"Most Frequent distance is \"+str(mostFreqDistance)+\"km, \"+str(counter)+\" times \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write GroundTruth Data for users to file\n",
    "\n",
    "# First we will remove brackets \"(\" and \")\" from the groundtruth coordinates\n",
    "newUserLabel={}\n",
    "for x,y in userLabel.items():\n",
    "    newCoord = str(y).replace(\"(\",\"\")\n",
    "    newCoord = newCoord.replace(\")\",\"\")\n",
    "    newCoord = newCoord.replace(\",\",\"\")\n",
    "    \n",
    "    newUserLabel[x]=newCoord\n",
    "\n",
    "# Path to save groundtruth data to CSV   \n",
    "userLabel =\"C:/Users/dennis/Desktop/dataTestTrain/userLabel.csv\"\n",
    "with open(userLabel, 'w', newline=\"\",encoding=\"utf-8\")as file:\n",
    "    writer = csv.writer(file, delimiter =\";\")\n",
    "    for x,y in newUserLabel.items():     \n",
    "        writer.writerow([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split user ids which have geotagged tweets in random training / test sets. 80/20 %\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train , test = train_test_split(data, test_size=0.2, random_state = 42)\n",
    "\n",
    "\n",
    "# Write test / train user ids in csv\n",
    "# Path to save the user ids into CSV for training or test dataset\n",
    "trainFile =\"C:/Users/dennis/Desktop/dataTestTrain/trainUser.csv\"\n",
    "testFile =\"C:/Users/dennis/Desktop/dataTestTrain/testUser.csv\"\n",
    "\n",
    "# Train users\n",
    "with open(trainFile, 'a', newline=\"\",encoding=\"utf-8\")as trainFile:\n",
    "    writer = csv.writer(trainFile, delimiter =\";\")\n",
    "    for id in train:     \n",
    "        writer.writerow([id])\n",
    "# Test Users\n",
    "with open(testFile, 'a', newline=\"\",encoding=\"utf-8\")as testFile:\n",
    "    writer = csv.writer(testFile, delimiter =\";\")\n",
    "    for id in test:\n",
    "        writer.writerow([id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT FOR GeoLocation 2400WORLD DATA FILE\n",
    "# GeoLocation needs a file containing coordinates which matches the users ground truth\n",
    "# to create the spatial grid for GeoLoc text.\n",
    "# Instead of the default file, we need to use the coordinates from our ground truth\n",
    "\n",
    "import csv\n",
    "\n",
    "# Path to ground truth data\n",
    "pathLabels =\"C:/Users/dennis/Desktop/dataTestTrain/userLabel.csv\"\n",
    "\n",
    "#We will extract only the coordinates\n",
    "trainingLabel=[]\n",
    "with open(pathLabels,encoding=\"utf8\",errors='ignore') as file:\n",
    "    for x in file:\n",
    "        data = x.split(\"\\t\")\n",
    "        coord=data[1]\n",
    "        coord=coord.replace(\" \",\",\")\n",
    "        trainingLabel.append(coord)     \n",
    "\n",
    "# Path to save the coordinates\n",
    "pathCoordinates = \"C:/Users/dennis/Desktop/dataTestTrain/2400_median_clustered_world.train\"\n",
    "with open(pathCoordinates, 'w', newline=\"\",encoding=\"utf-8\")as trainingFile:\n",
    "    writer = csv.writer(trainingFile, delimiter =\"\\t\",escapechar=' ', quoting=csv.QUOTE_NONE)\n",
    "    for x in trainingLabel:     \n",
    "        writer.writerow([x.replace('\\n',\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Creating Train / Test input datasets of users in format for GeoLocation ##############################\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "#\"train\" or \"test\"\n",
    "usage=\"train\"\n",
    "\n",
    "# Path for hydrated TweetsCOV19\n",
    "pathHydrated = \"C:/Users/dennis/Desktop/hydrated.jsonl\"\n",
    "\n",
    "# Path for CSV with training User ids or testing user ids\n",
    "pathUsers=\"C:/Users/dennis/Desktop/dataTestTrain/\"+usage+\"User.csv\"\n",
    "users=[]\n",
    "with open(pathUsers,encoding=\"utf8\",errors='ignore') as file:\n",
    "    for x in file:\n",
    "        user = x.replace(\"\\n\",\"\")\n",
    "        users.append(user)\n",
    "\n",
    "with open(pathHydrated,encoding=\"utf8\",errors='ignore') as hydratedJSONL:\n",
    "    labels ={}\n",
    "    for raw in hydratedJSONL:\n",
    "        tweet = json.loads(raw)\n",
    "        user = tweet[\"user\"][\"id_str\"]\n",
    "        text = tweet[\"full_text\"]\n",
    "        \n",
    "        # Check every tweet if user is in the selected dataset and if tweet is geo-tagged\n",
    "        # We will concatenate the tweet message if the user is in dataset and the tweet geo-tagged. As coordinates\n",
    "        # we use the most frequent city coordinates in variable newUserLabel (from 3 Cells above ) for the user\n",
    "        if (tweet[\"place\"]) == None and (tweet[\"geo\"])==None:\n",
    "            continue\n",
    "        if user not in users:\n",
    "            continue\n",
    "                       \n",
    "        else:\n",
    "            geo = tweet[\"geo\"]\n",
    "            if geo != None:\n",
    "                if user not in labels.keys():\n",
    "                    coord=newUserLabel[user]\n",
    "                    coord=coord.split(\" \")\n",
    "                    labels[user]=user+\"\\t\"+coord[0]+\"\\t\"+coord[1]+\"\\t\"+\" ||| \"+text\n",
    "                    continue\n",
    "                else:\n",
    "                    labels[user]=labels[user]+\" ||| \"+text\n",
    "                    continue\n",
    "               \n",
    "            if (tweet[\"place\"]!=None) and geo == None  :\n",
    "                if tweet[\"place\"][\"bounding_box\"] != None:\n",
    "                    if user not in labels.keys():\n",
    "                        coord=newUserLabel[user]\n",
    "                        coord=coord.split(\" \")\n",
    "                        labels[user]=user+\"\\t\"+coord[0]+\"\\t\"+coord[1]+\"\\t\"+\" ||| \"+text\n",
    "                        continue\n",
    "                    else:\n",
    "                        labels[user]=labels[user]+\" ||| \"+text\n",
    "                        continue\n",
    "\n",
    "                        \n",
    "# Path to save the dataset\n",
    "pathDataset=\"C:/Users/dennis/Desktop/dataTestTrain/geolocationUser.\"+usage\n",
    "with open(pathDataset, 'w', newline=\"\",encoding=\"utf-8\")as new:\n",
    "    writer = csv.writer(new, delimiter =\"\\t\",escapechar=' ', quoting=csv.QUOTE_NONE)   \n",
    "    for line in labels.values():\n",
    "        writer.writerow([line.replace(\"\\n\",\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will select ground truth data for DeepGeo and create the corresponsing file with coordinates\n",
    "\n",
    "import json,csv\n",
    "\n",
    "# Path for hydrated TweetsCOV19\n",
    "pathHydrated = \"C:/Users/dennis/Desktop/hydrated.jsonl\"\n",
    "\n",
    "\n",
    "users=[]\n",
    "groundTruthUser={}\n",
    "\n",
    "# Path for csv with userid's which are in training split\n",
    "pathTrainUser=\"C:/Users/dennis/Desktop/dataTestTrain/trainUser.csv\"\n",
    "with open(fileTrainUser,encoding=\"utf8\",errors='ignore') as file:\n",
    "    for x in file:\n",
    "        user = x.replace(\"\\n\",\"\")\n",
    "        users.append(user)\n",
    "\n",
    "with open(hydrated,encoding=\"utf8\",errors='ignore') as hydratedJSONL:\n",
    "\n",
    "    print(\"Starting Reading JSON File\")\n",
    "    for raw in hydratedJSONL:\n",
    "        tweet = json.loads(raw)\n",
    "        tweetId = tweet[\"id_str\"]\n",
    "        user = tweet[\"user\"][\"id_str\"]\n",
    "        if (tweet[\"place\"]) == None and (tweet[\"geo\"])==None:\n",
    "            continue\n",
    "        if user not in users:\n",
    "            continue\n",
    "                       \n",
    "        else:\n",
    "            # Parameter Geo\n",
    "            geo = tweet[\"geo\"]\n",
    "            if geo != None:\n",
    "                coordinates = tweet[\"geo\"][\"coordinates\"]\n",
    "                groundTruthTweet[tweetId]=coordinates\n",
    "                \n",
    "            # Parameter Place   \n",
    "            if (tweet[\"place\"]!=None) and geo == None  :\n",
    "                if tweet[\"place\"][\"bounding_box\"] != None:\n",
    "                    coordinates = list(tweet[\"place\"][\"bounding_box\"][\"coordinates\"][0][0])\n",
    "                    coordinates = (coordinates[1],coordinates[0])\n",
    "                    groundTruthTweet[tweetId]= coordinates  \n",
    "                    \n",
    "                    \n",
    "# DeepGeo models accepts ground truth data for training in a seperate file with following form\n",
    "#{\"tweet_id\": \"547240490022617088\", \"tweet_city\": \"omaha-ne055-us\", \n",
    "#\"tweet_latitude\": \"41.782947\", \"tweet_longitude\": \"-95.287009\"}\n",
    "# Since we are using geo-tagged tweets we will also put the ground truth coordinates in the field \"tweet_city\"\n",
    "\n",
    "# Path for ground truth input data to train deepGeo models\n",
    "pathGroundTruth = \"C:/Users/dennis/Desktop/dataTestTrain/deepgeo/validTweet/label.tweet.json\"\n",
    "with open(pathGroundTruth, 'a') as file:\n",
    "    for tweetId,coord in groundTruthTweet.items():\n",
    "        data = {}\n",
    "        lat=coord[0]\n",
    "        long=coord[1]\n",
    "        \n",
    "        data[\"tweet_id\"]=tweetId\n",
    "        data[\"tweet_city\"]=str(lat)+\",\"+str(long)\n",
    "        data[\"tweet_latitude\"]=str(lat)\n",
    "        data[\"tweet_longitude\"]=str(long)\n",
    "        \n",
    "        json.dump(data,file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting input data with needed features from metadata for training and testing datasets\n",
    "\n",
    "import json,csv\n",
    "\n",
    "# Select dataset\n",
    "usage=\"test\"\n",
    "\n",
    "#Path hydrated TweetsCOV19\n",
    "pathHydrated = \"C:/Users/dennis/Desktop/hydrated.jsonl\"\n",
    "\n",
    "# Read user id's if tweets are for training of testing\n",
    "users=[]\n",
    "pathUser=\"C:/Users/dennis/Desktop/dataTestTrain/\"+usage+\"User.csv\"\n",
    "with open(pathUser,encoding=\"utf8\",errors='ignore') as file:\n",
    "    for x in file:\n",
    "        user = x.replace(\"\\n\",\"\")\n",
    "        users.append(user)\n",
    "\n",
    "# This method will append tweets in the new training/test file\n",
    "def writeToJson(data):\n",
    "    DeepGeoHydrated = \"C:/Users/dennis/Desktop/dataTestTrain/deepGeoUser.\"+usage\n",
    "    with open(DeepGeoHydrated, 'a') as file:\n",
    "        for line in data:\n",
    "            json.dump(line,file)\n",
    "            file.write('\\n')\n",
    "        \n",
    "\n",
    "# Features which we will use for each tweet\n",
    "col_1 = ['contributors', 'lang']\n",
    "col_2 = ['created_at', 'favorited']\n",
    "col_3 = ['in_reply_to_status_id_str', 'user', 'retweet_count', 'favorite_count']\n",
    "col_4 = ['retweeted', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'source', 'entities']\n",
    "\n",
    "\n",
    "validData=[]\n",
    "with open(pathHydrated,encoding=\"utf8\",errors='ignore') as allTweets:\n",
    "        part=0\n",
    "        fullData =[]\n",
    "        for raw in allTweets:\n",
    "            fullData=[]\n",
    "            tweet = json.loads(raw)\n",
    "            user = tweet[\"user\"][\"id_str\"]\n",
    "            \n",
    "            if user not in users:\n",
    "                continue\n",
    "                        \n",
    "            else:\n",
    "                geo = tweet[\"geo\"]\n",
    "                if geo != None:\n",
    "                    line={}\n",
    "                    for column in col_1:\n",
    "                        line[column]=tweet[column]\n",
    "\n",
    "                    line[\"text\"]=tweet[\"full_text\"]\n",
    "\n",
    "                    for column in col_2:\n",
    "                        line[column]=tweet[column]\n",
    "\n",
    "                    line[\"filter_level\"]=\"medium\"    \n",
    "                    line[\"hashed_tweet_id\"]=tweet[\"id_str\"]       \n",
    "                    line['in_reply_to_user_id_str']=tweet['in_reply_to_user_id_str']                                          \n",
    "                    line['truncated'] = tweet[\"truncated\"]                                          \n",
    "                    line['in_reply_to_status_id']= None\n",
    "\n",
    "                    for column in col_3:\n",
    "                        line[column] = tweet[column]\n",
    "\n",
    "                    line['hashed_user_id']=tweet[\"user\"][\"id_str\"]\n",
    "\n",
    "                    for column in col_4:\n",
    "                        line[column]=tweet[column]\n",
    "                    \n",
    "                    fullData.append(line)\n",
    "                    validData.append([tweet[\"id_str\"],newUserLabel[user]])\n",
    "                    writeToJson(fullData)\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                if (tweet[\"place\"]!=None) and geo == None  :\n",
    "                    if tweet[\"place\"][\"bounding_box\"] != None:\n",
    "                                           \n",
    "                        line={}\n",
    "                        for column in col_1:\n",
    "                            line[column]=tweet[column]\n",
    "\n",
    "                        line[\"text\"]=tweet[\"full_text\"]\n",
    "\n",
    "                        for column in col_2:\n",
    "                            line[column]=tweet[column]\n",
    "\n",
    "                        line[\"filter_level\"]=\"medium\"    \n",
    "                        line[\"hashed_tweet_id\"]=tweet[\"id_str\"]       \n",
    "                        line['in_reply_to_user_id_str']=tweet['in_reply_to_user_id_str']                                          \n",
    "                        line['truncated'] = tweet[\"truncated\"]                                          \n",
    "                        line['in_reply_to_status_id']= None\n",
    "\n",
    "                        for column in col_3:\n",
    "                            line[column] = tweet[column]\n",
    "\n",
    "                        line['hashed_user_id']=tweet[\"user\"][\"id_str\"]\n",
    "\n",
    "                        for column in col_4:\n",
    "                            line[column]=tweet[column]\n",
    "                            \n",
    "                        fullData.append(line)\n",
    "                        validData.append([tweet[\"id_str\"],newUserLabel[user]])\n",
    "                        writeToJson(fullData) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the 3 input files ( training , testing, and groundtruth) in the corresponding path as declared \"config.py\" and \n",
    "# set the mode in \"utils.py\"  as \"train\". To train a specfic model set the corresponding parameters in \"config.py\" and then \n",
    "# use command \"python geo_train.py\". The trained model should be in folder \"output\", and has to be selected \n",
    "# when testing datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
