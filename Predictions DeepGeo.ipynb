{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file to predict TweetsCOV19 with DeepGeo models. Due to high memory usage, the input file had to be\n",
    "# splitted in multiple files (250K Tweets per file)\n",
    "\n",
    "import json,csv\n",
    "\n",
    "# Path hydrated Tweets\n",
    "tweetJSON=\"C:/Users/dennis/Desktop/BachelorArbeit/All/HydratedALL.jsonl\"\n",
    "\n",
    "# Path for splitted input file\n",
    "fileInput=\"C:/Users/dennis/Desktop/Part/hydratedDeepGeoPart\"\n",
    "def writeToJson(data, part):\n",
    "    DeepGeoHydrated = fileInput+str(part)+\".json\"\n",
    "    with open(DeepGeoHydrated, 'a') as file:\n",
    "        for line in data:\n",
    "            json.dump(line,file)\n",
    "            file.write('\\n')\n",
    "        \n",
    "            \n",
    "col_1 = ['contributors', 'lang']\n",
    "col_2 = ['created_at', 'favorited']\n",
    "col_3 = ['in_reply_to_status_id_str', 'user', 'retweet_count', 'favorite_count']\n",
    "col_4 = ['retweeted', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'source', 'entities']\n",
    "count = 0\n",
    "with open(tweetJSON,encoding=\"utf8\",errors='ignore') as allTweets:\n",
    "        part=0\n",
    "        data =[]\n",
    "        for raw in allTweets:\n",
    "            \n",
    "            tweet = json.loads(raw)\n",
    "            line={}\n",
    "\n",
    "            for column in col_1:\n",
    "                line[column]=tweet[column]\n",
    "\n",
    "            line[\"text\"]=tweet[\"full_text\"]\n",
    "\n",
    "            for column in col_2:\n",
    "                line[column]=tweet[column]\n",
    "\n",
    "            line[\"filter_level\"]=\"medium\"    \n",
    "            line[\"hashed_tweet_id\"]=tweet[\"id_str\"]       \n",
    "            line['in_reply_to_user_id_str']=tweet['in_reply_to_user_id_str']                                          \n",
    "            line['truncated'] = tweet[\"truncated\"]                                          \n",
    "            line['in_reply_to_status_id']= None\n",
    "\n",
    "            for column in col_3:\n",
    "                line[column] = tweet[column]\n",
    "\n",
    "            line['hashed_user_id']=tweet[\"user\"][\"id_str\"]\n",
    "\n",
    "            for column in col_4:\n",
    "                line[column]=tweet[column]\n",
    "\n",
    "            data.append(line)\n",
    "            \n",
    "            if len(data)==250000:\n",
    "                writeToJson(data,part)\n",
    "                data=[]\n",
    "                part=part+1\n",
    "        \n",
    "        writeToJson(data,part)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepGeo models used via commands in console, the first file is the python file to test the datasets, the second file is the\n",
    "# selected pre-trained model, the third file is the input data ( in our case on of the splitted data) and the \n",
    "# last one is the label.tweet.json which pretends to have the ground truth for the input data (deepGeo always expect\n",
    "# ground truth data for accuracy calculation, which we obviously dont have for TweetsCOV19). Always check in utils.py if the\n",
    "# selected mode is corred : \"train\" to train a model or \"test\" to predict tweets\n",
    "\n",
    "# command to predict tweets from hydrated.json with deepGeo R400 Noise model:\n",
    "#----------------------------------------------------------------------------------------\n",
    "# python desktop/deepGeo/geo_testMOD.py -m desktop/deepGeo/models/deepgeo_R400_noise \n",
    "# -d desktop/Parts2/hydratedDeepGeoPart0.json -l desktop/deepGeo/label.tweet.json --print_attn\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions files\n",
    "\n",
    "# Path for new file with predictions\n",
    "newFile = \"C:/Users/dennis/Desktop/deepGeoALL.csv\"\n",
    "\n",
    "\n",
    "with open(newFile, 'a',newline='') as allCSV:\n",
    "    writer = csv.writer(allCSV, delimiter=';')\n",
    "    \n",
    "    # Path to predicted files\n",
    "    pathParts = \"C:/Users/dennis/Desktop/deepGeo+Noise/\"\n",
    "    \n",
    "    # Part 0-27\n",
    "    for i in range(28):\n",
    "        file = path+\"hydratedDeepGeoPart\"+str(i)+\".json.csv\"\n",
    "        \n",
    "        #splitting and writing\n",
    "        with open(file,encoding=\"utf8\",errors='ignore') as CSV:\n",
    "            for line in CSV:\n",
    "                data = line.split(\";\")\n",
    "                id = data[0]\n",
    "                locRaw = data[1].replace(\"\\n\",\"\").split(\"-\")\n",
    "                newLine=[id, locRaw[0],locRaw[1],locRaw[2]]\n",
    "                #newLine=[id, locRaw[0]]\n",
    "                writer.writerow(newLine)\n",
    "            print(\"Finished with Part \"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from DeepGeo models with pre-trained models are in form of \"city-stateCode-countryCode\". Therefore\n",
    "# it is important to match the string into the needed location information \n",
    "\n",
    "#Dict with 2letter countryCode : Country Name\n",
    "ISO3166 = {\n",
    "\t'AD': 'Andorra',\n",
    "\t'AE': 'United Arab Emirates',\n",
    "\t'AF': 'Afghanistan',\n",
    "\t'AG': 'Antigua & Barbuda',\n",
    "\t'AI': 'Anguilla',\n",
    "\t'AL': 'Albania',\n",
    "\t'AM': 'Armenia',\n",
    "\t'AN': 'Netherlands Antilles',\n",
    "\t'AO': 'Angola',\n",
    "\t'AQ': 'Antarctica',\n",
    "\t'AR': 'Argentina',\n",
    "\t'AS': 'American Samoa',\n",
    "\t'AT': 'Austria',\n",
    "\t'AU': 'Australia',\n",
    "\t'AW': 'Aruba',\n",
    "\t'AZ': 'Azerbaijan',\n",
    "\t'BA': 'Bosnia and Herzegovina',\n",
    "\t'BB': 'Barbados',\n",
    "\t'BD': 'Bangladesh',\n",
    "\t'BE': 'Belgium',\n",
    "\t'BF': 'Burkina Faso',\n",
    "\t'BG': 'Bulgaria',\n",
    "\t'BH': 'Bahrain',\n",
    "\t'BI': 'Burundi',\n",
    "\t'BJ': 'Benin',\n",
    "\t'AFRICA/PORTO':'Benin',\n",
    "\t'BM': 'Bermuda',\n",
    "\t'BN': 'Brunei Darussalam',\n",
    "\t'BO': 'Bolivia',\n",
    "\t'BR': 'Brazil',\n",
    "\t'BS': 'Bahama',\n",
    "\t'BT': 'Bhutan',\n",
    "\t'BU': 'Burma (no longer exists)',\n",
    "\t'BV': 'Bouvet Island',\n",
    "\t'BW': 'Botswana',\n",
    "\t'BY': 'Belarus',\n",
    "\t'BZ': 'Belize',\n",
    "\t'CA': 'Canada',\n",
    "\t'CC': 'Cocos (Keeling) Islands',\n",
    "\t'CF': 'Central African Republic',\n",
    "\t'CD': 'Congo',\n",
    "\t'CG': 'Congo',\n",
    "\t'CH': 'Switzerland',\n",
    "\t'CI': 'CÃ´te D\\'ivoire (Ivory Coast)',\n",
    "\t'CK': 'Cook Iislands',\n",
    "\t'CL': 'Chile',\n",
    "\t'CM': 'Cameroon',\n",
    "\t'CN': 'China',\n",
    "\t'CO': 'Colombia',\n",
    "\t'CR': 'Costa Rica',\n",
    "\t'CS': 'Czechoslovakia (no longer exists)',\n",
    "\t'CU': 'Cuba',\n",
    "\t'CV': 'Cape Verde',\n",
    "\t'CX': 'Christmas Island',\n",
    "    'CW': 'Netherlands Antilles',\n",
    "\t'CY': 'Cyprus',\n",
    "\t'CZ': 'Czech Republic',\n",
    "\t'DD': 'German Democratic Republic (no longer exists)',\n",
    "\t'DE': 'Germany',\n",
    "\t'DJ': 'Djibouti',\n",
    "\t'DK': 'Denmark',\n",
    "\t'DM': 'Dominica',\n",
    "\t'DO': 'Dominican Republic',\n",
    "\t'DZ': 'Algeria',\n",
    "\t'EC': 'Ecuador',\n",
    "\t'EE': 'Estonia',\n",
    "\t'EG': 'Egypt',\n",
    "\t'EH': 'Western Sahara',\n",
    "\t'ER': 'Eritrea',\n",
    "\t'ES': 'Spain',\n",
    "\t'ET': 'Ethiopia',\n",
    "\t'FI': 'Finland',\n",
    "\t'FJ': 'Fiji',\n",
    "\t'FK': 'Falkland Islands (Malvinas)',\n",
    "\t'FM': 'Micronesia',\n",
    "\t'FO': 'Faroe Islands',\n",
    "\t'FR': 'France',\n",
    "\t'FX': 'France, Metropolitan',\n",
    "\t'GA': 'Gabon',\n",
    "\t'GB': 'United Kingdom',\n",
    "\t'GD': 'Grenada',\n",
    "\t'GE': 'Georgia',\n",
    "\t'GF': 'French Guiana',\n",
    "\t'GH': 'Ghana',\n",
    "\t'GI': 'Gibraltar',\n",
    "\t'GL': 'Greenland',\n",
    "\t'GM': 'Gambia',\n",
    "\t'GN': 'Guinea',\n",
    "\t'GP': 'Guadeloupe',\n",
    "\t'GQ': 'Equatorial Guinea',\n",
    "\t'GR': 'Greece',\n",
    "\t'GS': 'South Georgia and the South Sandwich Islands',\n",
    "\t'GT': 'Guatemala',\n",
    "\t'GU': 'Guam',\n",
    "\t'GW': 'Guinea-Bissau',\n",
    "\t'GY': 'Guyana',\n",
    "\t'HK': 'Hong Kong',\n",
    "\t'HM': 'Heard & McDonald Islands',\n",
    "\t'HN': 'Honduras',\n",
    "\t'HR': 'Croatia',\n",
    "\t'HT': 'Haiti',\n",
    "\t'HU': 'Hungary',\n",
    "\t'ID': 'Indonesia',\n",
    "\t'IE': 'Ireland',\n",
    "\t'IL': 'Israel',\n",
    "\t'IN': 'India',\n",
    "\t'IO': 'British Indian Ocean Territory',\n",
    "\t'IQ': 'Iraq',\n",
    "\t'IR': 'Islamic Republic of Iran',\n",
    "\t'IS': 'Iceland',\n",
    "\t'IT': 'Italy',\n",
    "\t'JM': 'Jamaica',\n",
    "\t'JO': 'Jordan',\n",
    "\t'JP': 'Japan',\n",
    "\t'KE': 'Kenya',\n",
    "\t'KG': 'Kyrgyzstan',\n",
    "\t'KH': 'Cambodia',\n",
    "\t'KI': 'Kiribati',\n",
    "\t'KM': 'Comoros',\n",
    "\t'KN': 'St. Kitts and Nevis',\n",
    "\t'KP': 'Korea, Democratic People\\'s Republic of',\n",
    "\t'KR': 'Korea, Republic of',\n",
    "\t'KW': 'Kuwait',\n",
    "\t'KY': 'Cayman Islands',\n",
    "\t'KZ': 'Kazakhstan',\n",
    "\t'LA': 'Lao People\\'s Democratic Republic',\n",
    "\t'LB': 'Lebanon',\n",
    "\t'LC': 'Saint Lucia',\n",
    "\t'LI': 'Liechtenstein',\n",
    "\t'LK': 'Sri Lanka',\n",
    "\t'LR': 'Liberia',\n",
    "\t'LS': 'Lesotho',\n",
    "\t'LT': 'Lithuania',\n",
    "\t'LU': 'Luxembourg',\n",
    "\t'LV': 'Latvia',\n",
    "\t'LY': 'Libyan Arab Jamahiriya',\n",
    "\t'MA': 'Morocco',\n",
    "\t'MC': 'Monaco',\n",
    "\t'MD': 'Moldova, Republic of',\n",
    "    'ME': 'Montenegro',\n",
    "\t'MG': 'Madagascar',\n",
    "\t'MH': 'Marshall Islands',\n",
    "\t'MK': 'North Macedonia',\n",
    "\t'ML': 'Mali',\n",
    "\t'MN': 'Mongolia',\n",
    "\t'MM': 'Myanmar',\n",
    "\t'MO': 'Macau',\n",
    "\t'MP': 'Northern Mariana Islands',\n",
    "\t'MQ': 'Martinique',\n",
    "\t'MR': 'Mauritania',\n",
    "\t'MS': 'Monserrat',\n",
    "\t'MT': 'Malta',\n",
    "\t'MU': 'Mauritius',\n",
    "\t'MV': 'Maldives',\n",
    "\t'MW': 'Malawi',\n",
    "\t'MX': 'Mexico',\n",
    "\t'MY': 'Malaysia',\n",
    "\t'MZ': 'Mozambique',\n",
    "\t'NA': 'Namibia',\n",
    "\t'NC': 'New Caledonia',\n",
    "\t'NE': 'Niger',\n",
    "\t'NF': 'Norfolk Island',\n",
    "\t'NG': 'Nigeria',\n",
    "\t'NI': 'Nicaragua',\n",
    "\t'NL': 'Netherlands',\n",
    "\t'NO': 'Norway',\n",
    "\t'NP': 'Nepal',\n",
    "\t'NR': 'Nauru',\n",
    "\t'NT': 'Neutral Zone (no longer exists)',\n",
    "\t'NU': 'Niue',\n",
    "\t'NZ': 'New Zealand',\n",
    "\t'OM': 'Oman',\n",
    "\t'PA': 'Panama',\n",
    "\t'PE': 'Peru',\n",
    "\t'PF': 'French Polynesia',\n",
    "\t'PG': 'Papua New Guinea',\n",
    "\t'PH': 'Philippines',\n",
    "\t'PK': 'Pakistan',\n",
    "\t'PL': 'Poland',\n",
    "    'PS': 'Palestine',\n",
    "\t'PM': 'St. Pierre & Miquelon',\n",
    "\t'PN': 'Pitcairn',\n",
    "\t'PR': 'Puerto Rico',\n",
    "\t'PT': 'Portugal',\n",
    "\t'PW': 'Palau',\n",
    "\t'PY': 'Paraguay',\n",
    "\t'QA': 'Qatar',\n",
    "\t'RE': 'RÃ©union',\n",
    "\t'RO': 'Romania',\n",
    "\t'RU': 'Russian Federation',\n",
    "\t'RS': 'Serbia',\n",
    "\t'RW': 'Rwanda',\n",
    "\t'SA': 'Saudi Arabia',\n",
    "\t'SB': 'Solomon Islands',\n",
    "\t'SC': 'Seychelles',\n",
    "\t'SD': 'Sudan',\n",
    "\t'SE': 'Sweden',\n",
    "\t'SG': 'Singapore',\n",
    "\t'SH': 'St. Helena',\n",
    "\t'SI': 'Slovenia',\n",
    "\t'SJ': 'Svalbard & Jan Mayen Islands',\n",
    "\t'SK': 'Slovakia',\n",
    "\t'SL': 'Sierra Leone',\n",
    "\t'SM': 'San Marino',\n",
    "\t'SN': 'Senegal',\n",
    "\t'SO': 'Somalia',\n",
    "\t'SR': 'Suriname',\n",
    "\t'ST': 'Sao Tome & Principe',\n",
    "    'SS': 'South Sudan',\n",
    "\t'SU': 'Union of Soviet Socialist Republics (no longer exists)',\n",
    "\t'SV': 'El Salvador',\n",
    "\t'SY': 'Syrian Arab Republic',\n",
    "\t'SZ': 'Swaziland',\n",
    "\t'TC': 'Turks & Caicos Islands',\n",
    "\t'TD': 'Chad',\n",
    "\t'TF': 'French Southern Territories',\n",
    "\t'TG': 'Togo',\n",
    "\t'TH': 'Thailand',\n",
    "\t'TJ': 'Tajikistan',\n",
    "\t'TK': 'Tokelau',\n",
    "    'TL': 'Timor Leste',\n",
    "\t'TM': 'Turkmenistan',\n",
    "\t'TN': 'Tunisia',\n",
    "\t'TO': 'Tonga',\n",
    "\t'TP': 'East Timor',\n",
    "\t'TR': 'Turkey',\n",
    "\t'TT': 'Trinidad & Tobago',\n",
    "\t'TV': 'Tuvalu',\n",
    "\t'TW': 'Taiwan, Province of China',\n",
    "\t'TZ': 'Tanzania, United Republic of',\n",
    "\t'UA': 'Ukraine',\n",
    "\t'UG': 'Uganda',\n",
    "\t'UM': 'United States Minor Outlying Islands',\n",
    "\t'US': 'United States',\n",
    "\t'UY': 'Uruguay',\n",
    "\t'UZ': 'Uzbekistan',\n",
    "\t'VA': 'Vatican City State (Holy See)',\n",
    "\t'VC': 'St. Vincent & the Grenadines',\n",
    "\t'VE': 'Venezuela',\n",
    "\t'VG': 'British Virgin Islands',\n",
    "\t'VI': 'United States Virgin Islands',\n",
    "\t'VN': 'Viet Nam',\n",
    "\t'VU': 'Vanuatu',\n",
    "\t'WF': 'Wallis & Futuna Islands',\n",
    "\t'WS': 'Samoa',\n",
    "\t'YD': 'Democratic Yemen (no longer exists)',\n",
    "\t'YE': 'Yemen',\n",
    "\t'YT': 'Mayotte',\n",
    "\t'YU': 'Yugoslavia',\n",
    "\t'ZA': 'South Africa',\n",
    "    'XK': 'Kosovo',\n",
    "\t'ZM': 'Zambia',\n",
    "\t'ZR': 'Zaire',\n",
    "\t'ZW': 'Zimbabwe',\n",
    "\t'ZZ': 'Unknown or unspecified country',\n",
    "}\n",
    "us_code ={\n",
    "    \"AL\": \"Alabama\",\n",
    "    \"AK\": \"Alaska\",\n",
    "    \"AS\": \"American Samoa\",\n",
    "    \"AZ\": \"Arizona\",\n",
    "    \"AR\": \"Arkansas\",\n",
    "    \"CA\": \"California\",\n",
    "    \"CO\": \"Colorado\",\n",
    "    \"CT\": \"Connecticut\",\n",
    "    \"DE\": \"Delaware\",\n",
    "    \"DC\": \"District Of Columbia\",\n",
    "    \"FM\": \"Federated States Of Micronesia\",\n",
    "    \"FL\": \"Florida\",\n",
    "    \"GA\": \"Georgia\",\n",
    "    \"GU\": \"Guam\",\n",
    "    \"HI\": \"Hawaii\",\n",
    "    \"ID\": \"Idaho\",\n",
    "    \"IL\": \"Illinois\",\n",
    "    \"IN\": \"Indiana\",\n",
    "    \"IA\": \"Iowa\",\n",
    "    \"KS\": \"Kansas\",\n",
    "    \"KY\": \"Kentucky\",\n",
    "    \"LA\": \"Louisiana\",\n",
    "    \"ME\": \"Maine\",\n",
    "    \"MH\": \"Marshall Islands\",\n",
    "    \"MD\": \"Maryland\",\n",
    "    \"MA\": \"Massachusetts\",\n",
    "    \"MI\": \"Michigan\",\n",
    "    \"MN\": \"Minnesota\",\n",
    "    \"MS\": \"Mississippi\",\n",
    "    \"MO\": \"Missouri\",\n",
    "    \"MT\": \"Montana\",\n",
    "    \"NE\": \"Nebraska\",\n",
    "    \"NV\": \"Nevada\",\n",
    "    \"NH\": \"New Hampshire\",\n",
    "    \"NJ\": \"New Jersey\",\n",
    "    \"NM\": \"New Mexico\",\n",
    "    \"NY\": \"New York\",\n",
    "    \"NC\": \"North Carolina\",\n",
    "    \"ND\": \"North Dakota\",\n",
    "    \"MP\": \"Northern Mariana Islands\",\n",
    "    \"OH\": \"Ohio\",\n",
    "    \"OK\": \"Oklahoma\",\n",
    "    \"OR\": \"Oregon\",\n",
    "    \"PW\": \"Palau\",\n",
    "    \"PA\": \"Pennsylvania\",\n",
    "    \"PR\": \"Puerto Rico\",\n",
    "    \"RI\": \"Rhode Island\",\n",
    "    \"SC\": \"South Carolina\",\n",
    "    \"SD\": \"South Dakota\",\n",
    "    \"TN\": \"Tennessee\",\n",
    "    \"TX\": \"Texas\",\n",
    "    \"UT\": \"Utah\",\n",
    "    \"VT\": \"Vermont\",\n",
    "    \"VI\": \"Virgin Islands\",\n",
    "    \"VA\": \"Virginia\",\n",
    "    \"WA\": \"Washington\",\n",
    "    \"WV\": \"West Virginia\",\n",
    "    \"WI\": \"Wisconsin\",\n",
    "    \"WY\": \"Wyoming\"\n",
    "}\n",
    "\n",
    "# The regioncode list is important to get the full state name for a prediction\n",
    "# Create Dict with Regioncode -> Regionname\n",
    "fileRegionCodes=\"C:/Users/Dennis/Desktop/deepgeo/regionCodes.csv\"\n",
    "\n",
    "#dictRegion is the variable we want to work with\n",
    "dictRegion={}\n",
    "listCountries=[]\n",
    "l={}\n",
    "with open(fileRegionCodes, encoding=\"utf8\",errors='ignore') as countryCodes:\n",
    "    for line in countryCodes:\n",
    "        data =line.split(\",\")\n",
    "        country = data[0].lower()\n",
    "        regionCode=data[1].lower()\n",
    "        regionName=data[2].replace('\"','').replace(\"\\n\",\"\")\n",
    "        \n",
    "        if country ==\"ca\":\n",
    "            regions ={\"ab\":\"01\",\"bc\":\"02\",\"mb\":\"03\",\"nb\":\"04\",\"nl\":\"05\",\"ns\":\"07\",\"on\":\"08\",\"pe\":\"09\",\"qc\":\"10\",\"sk\":\"11\",\"yt\":\"12\",\"nt\":\"13\",\"nu\":\"14\"}\n",
    "            regionCode = regions[regionCode]\n",
    "        if country not in listCountries :\n",
    "            if len(l )> 0:\n",
    "                currCountry = listCountries[-1]\n",
    "                dictRegion[currCountry]=l\n",
    "                l={}\n",
    "            listCountries.append(country)\n",
    "            l[regionCode]=regionName\n",
    "            \n",
    "        if country in listCountries:\n",
    "            l[regionCode]=regionName\n",
    "            \n",
    "\n",
    "import csv\n",
    "\n",
    "# Path to file with original prediction \"city-stateCode-CountryCode\"\n",
    "pathResultsRaw = \"C:/Users/dennis/Desktop/deepGeoAll.csv\"\n",
    "\n",
    "# Path for new file with predictions in form of \"id\";\"country\";\"state\";\"city\"\n",
    "pathResultsFinal = \"C:/Users/dennis/Desktop/deepGeo+Noise.csv\"\n",
    "\n",
    "with open(pathResultsFinal, 'a',newline='') as finalCSV:\n",
    "    writer = csv.writer(finalCSV, delimiter=';')    \n",
    "    #writer.writerow(newLine)       \n",
    "    with open(pathResultsRaw,encoding=\"utf8\") as rawCSV:\n",
    "        for line in rawCSV:\n",
    "            data = line.split(\";\")\n",
    "            id = data[0]\n",
    "            city = data[1]\n",
    "            regionCode = data[2]\n",
    "            country =data[3].replace(\"\\n\",\"\")\n",
    "            \n",
    "            if country == \"us\":\n",
    "                region=dictRegion[country][regionCode[:2]]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "            if country == \"de\":\n",
    "                regionCode = regionCode[:2]\n",
    "                region=dictRegion[country][regionCode]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "            if country ==\"fr\":\n",
    "                regionCode = regionCode[:2]\n",
    "                region=dictRegion[country][regionCode]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "            if country ==\"es\":\n",
    "                regionCode = regionCode[:2]\n",
    "                region=dictRegion[country][regionCode]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "            if country ==\"in\":\n",
    "                region=dictRegion[country][regionCode]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "            if country ==\"it\":\n",
    "                regionCode = regionCode[:2]\n",
    "                region=dictRegion[country][regionCode]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                region = dictCountry[country][city]\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)\n",
    "            except:\n",
    "                region=\"\"\n",
    "                if country == \"de\":\n",
    "                    falseName =[\"neu ulm\",\"luebeck\",\"rheinfelden (baden)\",\"wuerzburg\",\"halle neustadt\"]\n",
    "                    trueName=[\"neu-ulm\",\"lÃ¼beck\",\"rheinfelden\",\"wÃ¼rzburg\",\"halle\"]\n",
    "                    index = falseName.index(city)\n",
    "                    city =  trueName[index]\n",
    "                    region = dictCountry[country][city]\n",
    "                \n",
    "                if country == \"es\":\n",
    "                    falseName=[\"vitoria gasteiz\",\"castello de la plana\",\"ourense\"]\n",
    "                    trueName=[\"vitoria-gasteiz\",\"castellon de la plana\",\"orense\"]\n",
    "                    index = falseName.index(city)\n",
    "                    city =  trueName[index]\n",
    "                    region = dictCountry[country][city]\n",
    "                \n",
    "                if country ==\"fr\":\n",
    "                    falseName=[\"boulogne billancourt\",\"clermont ferrand\",\"la roche sur yon\",\"chalon sur saone\"]\n",
    "                    trueName=[\"boulogne-billancourt\",\"clermont-ferrand\",\"la roche-sur-yon\",\"chalons sur saone\"]\n",
    "                    index = falseName.index(city)\n",
    "                    city =  trueName[index]\n",
    "                    region = dictCountry[country][city]\n",
    "                \n",
    "                if country ==\"in\":\n",
    "                    if city ==\"greater noida\":\n",
    "                        region=\"Uttar Pradesh\"\n",
    "                \n",
    "                if country==\"us\":\n",
    "                    falseName=[\"san tan valley\",\"lexington fayette\",\"south bel air\",\"staten island\",\n",
    "                               \"borough of bronx\",\"washington, d. c.\",\"winston salem\",\"new york city\"]\n",
    "                    trueName=[\"san tan mobile village\",\"lexington hills\",\"bel air\",\"staten island junction\",\n",
    "                              \"borough park\",\"washington city\",\"winston-salem\",\"new york\"]\n",
    "                    index = falseName.index(city)\n",
    "                    city =  trueName[index]\n",
    "                    region = dictCountry[country][city]\n",
    "                    \n",
    "\n",
    "                if city ==\"new york city\":\n",
    "                    region =\"New York\"\n",
    "                    countNewYork=countNewYork+1\n",
    "                country= ISO3166[country.upper()]\n",
    "                newLine=[id,country,region,city]\n",
    "                writer.writerow(newLine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
