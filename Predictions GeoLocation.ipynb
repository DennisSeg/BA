{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Path to hydrated TweetsCOV19\n",
    "pathHydrated = \"C:/Users/dennis/Desktop/hydrated.jsonl\"\n",
    "\n",
    "# In order to create the input for GeoLocation models, we will extract users and concatenate their text messages\n",
    "data ={}\n",
    "with open(pathHydrated,encoding=\"utf8\",errors='ignore') as hydratedJSONL:\n",
    "    count = 0\n",
    "    for raw in hydratedJSONL:\n",
    "        tweet = json.loads(raw)\n",
    "        user = tweet[\"user\"][\"id_str\"]\n",
    "        #user = tweet[\"user\"][\"screen_name\"]\n",
    "        text = tweet[\"full_text\"]\n",
    "\n",
    "        if user not in data.keys():\n",
    "            data[user]=\" ||| \"+text\n",
    "        elif user in data.keys():\n",
    "            data[user] = data[user]+\" ||| \"+text\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "        if (count %100000) == 0:\n",
    "            print(\"Finished : \"+str(count)+\" Tweets /// \"+str(len(data)))\n",
    "        count = count+1\n",
    "        \n",
    "        \n",
    "# Path for input file\n",
    "pathInput =\"C:/Users/dennis/Desktop/geoLocation/user_info.test\"\n",
    "\n",
    "with open(pathInput, 'w', newline=\"\",encoding=\"utf-8\")as newFile:\n",
    "    writer = csv.writer(newFile, delimiter =\"\\t\") \n",
    "    for key,value in data.items():\n",
    "        user=key+\"\\t\"+str(0.0)+\"\\t\"+str(0.0)+\"\\t\"+value.replace(\"\\n\",\"\")\n",
    "        coord = \"0.0\"\n",
    "        writer.writerow([key,coord,coord,value.replace(\"\\n\",'')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next cell is the config file for GeoLocation models. The test file to predict tweets\n",
    "# should be zipped in a .gz file. You can put the selected model in \"models_to_run\"\n",
    "# and the path to the files.  !!! GEOLOCATION IS WRITTEN IN PYTHON 2.7  !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 26 Feb 2016\n",
    "\n",
    "@author: af\n",
    "'''\n",
    "from os import path\n",
    "all_models = ['text_classification', 'network_lp_regression', 'network_lp_regression_collapsed', 'network_lp_classification']\n",
    "models_to_run = ['network_lp_regression']\n",
    "if 'text_classification' not in models_to_run and 'network_lp_classification' not in models_to_run:\n",
    "    do_not_discretize = True\n",
    "else:\n",
    "    do_not_discretize = False\n",
    "\n",
    "DATASET_NUMBER = 3\n",
    "TEXT_ONLY = False\n",
    "DATA_HOME = 'C:/Users/Dennis/Desktop/geolocation/'\n",
    "DATASETS = ['cmu', 'na', 'world']\n",
    "ENCODINGS = ['latin1', 'utf-8', 'utf-8']\n",
    "buckets = [300 , 2400, 2400]\n",
    "reguls = [5e-5, 1e-6, 2e-7]\n",
    "celeb_thresholds = [5 , 15, 15]\n",
    "BUCKET_SIZE = buckets[DATASET_NUMBER - 1]\n",
    "celeb_threshold = celeb_thresholds[DATASET_NUMBER - 1]\n",
    "GEOTEXT_HOME = path.join(DATA_HOME, DATASETS[DATASET_NUMBER - 1])\n",
    "data_encoding = ENCODINGS[DATASET_NUMBER - 1]\n",
    "users_home = path.join(\"C:/Users/Dennis/Desktop/geolocation/\", 'processed_data')\n",
    "testfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/tweetCov/\", 'user_info.test.gz')\n",
    "devfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/tweetCov/\", 'user_info.dev.gz')\n",
    "trainfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/tweetCov/\", 'user_info.train.gz')\n",
    "\n",
    "#testfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/test/\", 'user_info.test.gz')\n",
    "#devfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/test/\", 'user_info.dev.gz')\n",
    "#trainfile = path.join(\"C:/Users/Dennis/Desktop/geolocation/test/\", 'user_info.train.gz')\n",
    "\n",
    "print \"dataset: \" + DATASETS[DATASET_NUMBER - 1]\n",
    "lngs = []\n",
    "ltts = []\n",
    "pointText = {}\n",
    "keys = []\n",
    "userFirstTime = {}\n",
    "userLocation = {}\n",
    "locationUser = {}\n",
    "userlat = {}\n",
    "userlon = {}\n",
    "testUsers = {}\n",
    "testUsers2 ={} \n",
    "trainUsers = {}\n",
    "devUsers = {}\n",
    "classLatMedian = {}\n",
    "classLonMedian = {}\n",
    "classLatMean = {}\n",
    "classLonMean = {}\n",
    "trainClasses = {}\n",
    "devClasses = {}\n",
    "testClasses = {}\n",
    "categories = []\n",
    "mentions = []\n",
    "testText = {}\n",
    "devText = {}\n",
    "trainText = {}\n",
    "\n",
    "\n",
    "\n",
    "X_train = None\n",
    "X_dev = None\n",
    "X_test = None\n",
    "Y_train = None\n",
    "Y_dev = None\n",
    "Y_test = None\n",
    "U_train = None\n",
    "U_dev = None\n",
    "U_test = None\n",
    "\n",
    "\n",
    "n_comp = 500\n",
    "factorizers = []\n",
    "results = {}\n",
    "mention_graph = None\n",
    "partitionMethod = 'median'\n",
    "partitionMethods = ['kmeans', 'ward', 'average', 'complete', 'median','spectral', 'kmeans', 'meanShift', 'Birch']\n",
    "binary = False\n",
    "sublinear=True\n",
    "penalty = 'l1'\n",
    "fit_intercept = True\n",
    "norm = 'l2'\n",
    "use_idf = True\n",
    "node_orders = ['l2h', 'h2l', 'random']\n",
    "feature_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is the model file for GeoLocation, the parameter for the models can be set in the last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 26 Feb 2016\n",
    "\n",
    "@author: af\n",
    "'''\n",
    "'''\n",
    "Created on 4 Sep 2014\n",
    "\n",
    "@author: af\n",
    "'''\n",
    "#from params import *\n",
    "from IPython.core.debugger import Tracer\n",
    "from collections import defaultdict, Counter\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import gzip\n",
    "from haversine import haversine\n",
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from scipy import mean\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils.extmath import density\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "\n",
    "\n",
    "print str(datetime.now())\n",
    "script_start_time = time.time()\n",
    "\n",
    "\n",
    "dumping_preds=[]           \n",
    "LPpredictions={}            \n",
    "    \n",
    "def median(mylist):\n",
    "    return np.median(mylist)\n",
    "    '''\n",
    "    my implementation of median\n",
    "    sorts = sorted(mylist)\n",
    "    length = len(sorts)\n",
    "    if not length % 2:\n",
    "        return (sorts[length / 2] + sorts[length / 2 - 1]) / 2.0\n",
    "    return sorts[length / 2]\n",
    "    '''\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    '''\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "\n",
    "    # 6367 km is the radius of the Earth\n",
    "    km = 6367 * c\n",
    "    '''\n",
    "    point1 = (lat1, lon1)\n",
    "    point2 = (lat2, lon2)\n",
    "    \n",
    "    return haversine(point1, point2)\n",
    "\n",
    "\n",
    "def users(file, type='train', write=False, readText=True):\n",
    "    if readText:\n",
    "        print(\"Text is being read.\")\n",
    "        if TEXT_ONLY:\n",
    "            print('mentions are removed.')\n",
    "\n",
    "    # with codecs.open(file, 'r', encoding=data_encoding) as inf:\n",
    "    with gzip.open(file, 'r') as inf:\n",
    "        for line in inf:\n",
    "            # print line\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) != 4:\n",
    "                print fields\n",
    "            user = fields[0].strip().lower()\n",
    "            lat = fields[1]\n",
    "            lon = fields[2]\n",
    "            if readText:\n",
    "                text = fields[3].strip()\n",
    "            if TEXT_ONLY and readText:\n",
    "                text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "            locStr = lat + ',' + lon\n",
    "            locFloat = (float(lat), float(lon))\n",
    "            userLocation[user] = locStr\n",
    "            if type == 'train':\n",
    "                trainUsers[user] = locStr\n",
    "                if readText:\n",
    "                    trainText[user] = text\n",
    "                users_in_loc = locationUser.get(locFloat, [])\n",
    "                users_in_loc.append(user)\n",
    "                locationUser[locFloat] = users_in_loc\n",
    "            elif type == 'test':\n",
    "                testUsers[user] = locStr\n",
    "                if readText:\n",
    "                    testText[user] = text\n",
    "            elif type == 'dev':\n",
    "                devUsers[user] = locStr\n",
    "                if readText:\n",
    "                    devText[user] = text\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createTrainDir(granularity, partitionMethod, create_dir=False):\n",
    "    filename='C:/Users/Dennis/Desktop/geolocation/world/processed_data/2400_median_clustered_world.train'\n",
    "    #filename = path.join(GEOTEXT_HOME, 'processed_data/' + str(granularity).strip() + '_' + partitionMethod + '_clustered.train')\n",
    "    print \"reading \" + filename\n",
    "    allpoints = []\n",
    "    allpointsMinLat = []\n",
    "    allpointsMaxLat = []\n",
    "    allpointsMinLon = []\n",
    "    allpointsMaxLon = []\n",
    "    with codecs.open(filename, 'r', encoding=data_encoding) as inf:\n",
    "        for line in inf:\n",
    "            points = []\n",
    "            minlat = 1000\n",
    "            maxlat = -1000\n",
    "            minlon = 1000\n",
    "            maxlon = -1000\n",
    "            fields = line.split('\\t')\n",
    "            points = [locationStr2Float(loc) for loc in fields]\n",
    "            \n",
    "            lats = [point[0] for point in points]\n",
    "            lons = [point[1] for point in points]\n",
    "            minlat = min(lats)\n",
    "            maxlat = max(lats)\n",
    "            minlon = min(lons)\n",
    "            maxlon = max(lons)\n",
    "            allpointsMinLat.append(minlat)\n",
    "            allpointsMaxLat.append(maxlat)\n",
    "            allpointsMaxLon.append(maxlon)\n",
    "            allpointsMinLon.append(minlon)\n",
    "            allpoints.append(points)\n",
    "\n",
    "    i = -1\n",
    "    for cluster in allpoints:\n",
    "        # create a directory\n",
    "        i += 1\n",
    "        lats = [location[0] for location in cluster]\n",
    "        longs = [location[1] for location in cluster]\n",
    "        medianlat = np.median(lats)\n",
    "        medianlon = np.median(longs)\n",
    "        meanlat = np.mean(lats)\n",
    "        meanlon = np.mean(longs)\n",
    "        label = str(i).strip()\n",
    "        categories.append(label)\n",
    "        classLatMedian[label] = medianlat\n",
    "        classLonMedian[label] = medianlon\n",
    "        classLatMean[label] = meanlat\n",
    "        classLonMean[label] = meanlon    \n",
    "        \n",
    "        #for location in cluster:\n",
    "        #    locusers = locationUser[(location[0], location[1])]\n",
    "        #    user_class = dict(zip(locusers, [i] * len(locusers)))\n",
    "        #    trainClasses.update(user_class)\n",
    "        for location in cluster:\n",
    "            try:\n",
    "                locusers = locationUser[(location[0], location[1])]\n",
    "                user_class = dict(zip(locusers, [i] * len(locusers)))\n",
    "                trainClasses.update(user_class)\n",
    "            except:\n",
    "                continue\n",
    "    print \"train directories created and class median and mean lat,lon computed. trainfile: \" + filename\n",
    "\n",
    "    devDistances = []\n",
    "    for user in devUsers:\n",
    "        locationStr = devUsers[user]\n",
    "        latlon = locationStr.split(',')\n",
    "        latitude = float(latlon[0])\n",
    "        longitude = float(latlon[1])\n",
    "        classIndex, dist = assignClass(latitude, longitude)\n",
    "        devDistances.append(dist)\n",
    "        devClasses[user] = int(classIndex)\n",
    "    \n",
    "    testDistances = []\n",
    "    for user in testUsers:\n",
    "        locationStr = testUsers[user]\n",
    "        latlon = locationStr.split(',')\n",
    "        latitude = float(latlon[0])\n",
    "        longitude = float(latlon[1])\n",
    "        classIndex, dist = assignClass(latitude, longitude)\n",
    "        testDistances.append(dist)\n",
    "        testClasses[user] = int(classIndex)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    Report_Ideal = False\n",
    "    if Report_Ideal:\n",
    "        print \"Ideal mean dev distance is \" + str(np.mean(devDistances))\n",
    "        print \"Ideal median dev distance is \" + str(np.median(devDistances))\n",
    "        print \"Ideal Acc@161 dev is \" + str(len([dist for dist in devDistances if dist < 161]) / (len(devDistances) + 0.0))\n",
    "        \n",
    "        print \"Ideal mean test distance is \" + str(np.mean(testDistances))\n",
    "        print \"Ideal median test distance is \" + str(np.median(testDistances))\n",
    "        print \"Ideal Acc@161 test is \" + str(len([dist for dist in testDistances if dist < 161]) / (len(testDistances) + 0.0))\n",
    "\n",
    "def assignClass(latitude, longitude):\n",
    "    '''\n",
    "    Given a coordinate find the class whose median is the closest point. Then return the index of that class.\n",
    "    This function can be used for parameter tuning with validation data and evaluation with test data.\n",
    "    '''\n",
    "    minDistance = 1000000\n",
    "    classIndex = -1\n",
    "    for i in classLatMedian:\n",
    "        lat = classLatMedian[str(i).strip()]\n",
    "        lon = classLonMedian[str(i).strip()]\n",
    "        dist = distance(latitude, longitude, lat, lon)\n",
    "        if dist < minDistance:\n",
    "            minDistance = dist\n",
    "            classIndex = i\n",
    "    return classIndex, minDistance\n",
    "\n",
    "\n",
    "\n",
    "def create_directories(granularity, partitionMethod, write=False):\n",
    "    createTrainDir(granularity, partitionMethod, write)\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(encoding=data_encoding)) for s in docs) / 1e6\n",
    "\n",
    "def evaluate(preds, U_test, categories, scores):\n",
    "    sumMeanDistance = 0\n",
    "    sumMedianDistance = 0\n",
    "    distances = []\n",
    "    confidences = []\n",
    "    randomConfidences = []\n",
    "    gmm = False\n",
    "    for i in range(0, len(preds)):\n",
    "        user = U_test[i]\n",
    "        location = userLocation[user].split(',')\n",
    "        lat = float(location[0])\n",
    "        lon = float(location[1])\n",
    "        # gaussian mixture model\n",
    "        if gmm:\n",
    "            sumMedianLat = 0\n",
    "            sumMedianLon = 0\n",
    "            sumMeanLat = 0\n",
    "            sumMeanLon = 0\n",
    "            numClasses = len(categories)\n",
    "            sortedScores = sorted(scores[i], reverse=True)\n",
    "            top1Score = sortedScores[0]\n",
    "            top2Score = sortedScores[1]\n",
    "            print top1Score\n",
    "            print top2Score\n",
    "            for c in range(0, numClasses):\n",
    "                score = scores[i][c]\n",
    "                category = categories[c]\n",
    "                medianlat = classLatMedian[category]  \n",
    "                medianlon = classLonMedian[category]  \n",
    "                meanlat = classLatMean[category] \n",
    "                meanlon = classLonMean[category]\n",
    "                sumMedianLat += score * medianlat\n",
    "                sumMedianLon += score * medianlon\n",
    "                sumMeanLat += score * meanlat\n",
    "                sumMeanLon += score * meanlon\n",
    "            distances.append(distance(lat, lon, sumMedianLat, sumMedianLon)) \n",
    "            \n",
    "        else:\n",
    "            prediction = categories[preds[i]]\n",
    "            if scores != None:\n",
    "                confidence = scores[i][preds[i]] \n",
    "                confidences.append(confidence)\n",
    "            medianlat = classLatMedian[prediction]  \n",
    "            medianlon = classLonMedian[prediction]  \n",
    "            meanlat = classLatMean[prediction] \n",
    "            meanlon = classLonMean[prediction]      \n",
    "            distances.append(distance(lat, lon, medianlat, medianlon))\n",
    "            sumMedianDistance = sumMedianDistance + distance(lat, lon, medianlat, medianlon)\n",
    "            sumMeanDistance = sumMeanDistance + distance(lat, lon, meanlat, meanlon)\n",
    "    # averageMeanDistance = sumMeanDistance / float(len(preds))\n",
    "    # averageMedianDistance = sumMedianDistance / float(len(preds))\n",
    "    # print \"Average mean distance is \" + str(averageMeanDistance)\n",
    "    # print \"Average median distance is \" + str(averageMedianDistance)\n",
    "    print \"Mean distance is \" + str(np.mean(distances))\n",
    "    print \"Median distance is \" + str(np.median(distances))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def error(predicted_label, user):\n",
    "    lat1, lon1 = locationStr2Float(userLocation[user])\n",
    "    lat2 = classLatMedian[predicted_label]  \n",
    "    lon2 = classLonMedian[predicted_label]\n",
    "    return distance(lat1, lon1, lat2, lon2)         \n",
    "\n",
    "def loss(preds, U_test, loss='median', save=True, save_results_bucket_size=False, results_key=None):\n",
    "    global results\n",
    "    if len(preds) != len(U_test): \n",
    "        print \"The number of test sample predictions is: \" + str(len(preds))\n",
    "        print \"The number of test samples is: \" + str(len(U_test))\n",
    "        print \"fatal error!\"\n",
    "        sys.exit()\n",
    "    sumMeanDistance = 0\n",
    "    sumMedianDistance = 0\n",
    "    distances = []\n",
    "    user_location = {}\n",
    "    acc = 0.0\n",
    "    center_of_us = (39.50, -98.35)\n",
    "    nyc = (40.7127, -74.0059)\n",
    "    la = (34.0500, -118.2500)\n",
    "    distances_from_nyc = []\n",
    "    distances_from_la = []\n",
    "    distances_from_center = []\n",
    "    heatmap_lats = []\n",
    "    heatmap_lons = []\n",
    "    heatmap_values = []\n",
    "    uniformer = defaultdict(int)\n",
    "    max_user_per_region = 50\n",
    "    class_error = defaultdict(list)\n",
    "    error_heatmap = False\n",
    "    for i in range(0, len(preds)):\n",
    "        user = U_test[i]\n",
    "        location = userLocation[user].split(',')\n",
    "        lat = float(location[0])\n",
    "        lon = float(location[1])\n",
    "        user_original_class, minDistance = assignClass(lat, lon)\n",
    "        distances_from_center.append(distance(lat, lon, center_of_us[0], center_of_us[1]))\n",
    "        distances_from_nyc.append(distance(lat, lon, nyc[0], nyc[1]))\n",
    "        distances_from_la.append(distance(lat, lon, la[0], la[1]))\n",
    "        # if preds[i] == int(testClasses[user]):\n",
    "        #    acc += 1\n",
    "        # print str(Y_test[i]) + \" \" + str(preds[i])\n",
    "        prediction = categories[preds[i]]\n",
    "        medianlat = classLatMedian[prediction]  \n",
    "        medianlon = classLonMedian[prediction]  \n",
    "        user_location[user] = (medianlat, medianlon)\n",
    "        meanlat = classLatMean[prediction] \n",
    "        meanlon = classLonMean[prediction]\n",
    "        predictionCoordinate = 'median'\n",
    "        if predictionCoordinate == 'median':      \n",
    "            dd = distance(lat, lon, medianlat, medianlon)\n",
    "            distances.append(dd)\n",
    "            if error_heatmap:\n",
    "                num_in_region = uniformer[str(int(lat)) + ',' + str(int(lon))]\n",
    "                heatmap_lats.append(lat)\n",
    "                heatmap_lons.append(lon)\n",
    "                heatmap_values.append(dd)\n",
    "                uniformer[str(int(lat)) + ',' + str(int(lon))] = num_in_region + 1\n",
    "                class_error[user_original_class].append(dd)\n",
    "            \n",
    "        elif predictionCoordinate == 'mean':\n",
    "            distances.append(distance(lat, lon, meanlat, meanlon))\n",
    "    \n",
    "    if save:\n",
    "        print \"dumping the results in preds.pkl\"\n",
    "        with open(path.join(GEOTEXT_HOME, 'predsTest.pkl'), 'wb') as outf:\n",
    "            pickle.dump(user_location, outf) \n",
    "\n",
    "    # print \"Average distance from class mean is \" + str(averageMeanDistance)\n",
    "    # print \"Average distance from class median is \" + str(averageMedianDistance)\n",
    "    acc_at_161 = 100 * len([d for d in distances if d < 161]) / float(len(distances))\n",
    "    print \"Mean: \" + str(int(np.mean(distances))) + \" Median: \" + str(int(np.median(distances))) + \" Acc@161: \" + str(int(acc_at_161))\n",
    "    print\n",
    "    extra_info = False\n",
    "    if extra_info:\n",
    "        print \"Mean distance from center of us is \" + str(int(np.mean(distances_from_center)))\n",
    "        print \"Median distance from center of us is \" + str(int(np.median(distances_from_center)))\n",
    "        print \"Mean distance from nyc is \" + str(int(np.mean(distances_from_nyc)))\n",
    "        print \"Median distance from nyc is \" + str(int(np.median(distances_from_nyc)))\n",
    "        print \"Mean distance from la is \" + str(int(np.mean(distances_from_la)))\n",
    "        print \"Median distance from la is \" + str(int(np.median(distances_from_la)))\n",
    "\n",
    "    return np.mean(distances), np.median(distances), acc_at_161\n",
    "\n",
    "def lossbycoordinates(coordinates):\n",
    "    if len(coordinates) != len(testUsers): \n",
    "        print \"The number of test sample predictions is: \" + str(len(coordinates))\n",
    "        print \"The number of test samples is: \" + str(len(testUsers))\n",
    "        print \"fatal error!\"\n",
    "        sys.exit()\n",
    "    sumMeanDistance = 0\n",
    "    sumMedianDistance = 0\n",
    "    distances = []\n",
    "    U = testUsers.keys()\n",
    "    for i in range(0, len(coordinates)):\n",
    "        user = U[i]\n",
    "        location = userLocation[user].split(',')\n",
    "        lat = float(location[0])\n",
    "        lon = float(location[1])\n",
    "        distances.append(distance(lat, lon, coordinates[i][0], coordinates[i][1]))\n",
    "        \n",
    "    # print \"Average distance from class mean is \" + str(averageMeanDistance)\n",
    "    # print \"Average distance from class median is \" + str(averageMedianDistance)\n",
    "    print \"Mean distance is \" + str(np.mean(distances))\n",
    "    print \"Median distance is \" + str(np.median(distances))\n",
    "    \n",
    "    if loss == 'median':\n",
    "        return np.median(distances)\n",
    "    elif loss == 'mean':\n",
    "        return np.mean(distances) \n",
    "\n",
    "def feature_extractor2(use_mention_dictionary=False, use_idf=True, norm='l2', binary=False, sublinear_tf=True, min_df=1, max_df=1.0, BuildCostMatrices=False, vectorizer=None, stop_words=None, novectorization=False, vocab=None):\n",
    "    '''\n",
    "    read train, dev and test dictionaries and extract textual features using tfidfvectorizer.\n",
    "    '''\n",
    "    \n",
    "    U_train = [u for u in sorted(trainUsers)]\n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"%d categories\" % len(categories))\n",
    "    print()\n",
    "    # split a training set and a test set\n",
    "    Y_train = np.asarray([trainClasses[u] for u in U_train])\n",
    "    Y_test = np.asarray([testClasses[u] for u in U_test])\n",
    "    Y_dev = np.asarray([devClasses[u] for u in U_dev])\n",
    " \n",
    "    logging.info(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if vectorizer == None:    \n",
    "        if use_mention_dictionary:\n",
    "            print \"using @ mention dictionary as vocab...\"\n",
    "            extract_mentions()\n",
    "            vectorizer = TfidfVectorizer(use_idf=use_idf, norm=norm, binary=binary, sublinear_tf=sublinear_tf, min_df=1, max_df=max_df, ngram_range=(1, 1), vocabulary=mentions, stop_words=stop_words)\n",
    "        else:\n",
    "            print \"mindf: \" + str(min_df) + \" maxdf: \" + str(max_df)\n",
    "            vectorizer = TfidfVectorizer(use_idf=use_idf, norm=norm, binary=binary, sublinear_tf=sublinear_tf, min_df=min_df, max_df=max_df, ngram_range=(1, 1), stop_words=stop_words, vocabulary=vocab, encoding=data_encoding)\n",
    "\n",
    "    X_train = vectorizer.fit_transform([trainText[u] for u in U_train])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    duration = time.time() - t0\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "    print()\n",
    "    \n",
    "    print(\"Extracting features from the dev dataset using the same vectorizer\")\n",
    "    t0 = time.time()\n",
    "    X_dev = vectorizer.transform([devText[u] for u in U_dev])\n",
    "    duration = time.time() - t0\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_dev.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"Extracting features from the test dataset using the same vectorizer\")\n",
    "    t0 = time.time()\n",
    "    X_test = vectorizer.transform([testText[u] for u in U_test])\n",
    "    duration = time.time() - t0\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "    print()\n",
    "            \n",
    "    return X_train, Y_train, U_train, X_dev, Y_dev, U_dev, X_test, Y_test, U_test, categories, feature_names\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def classify(X_train, Y_train, U_train, X_dev, Y_dev, U_dev, X_test, Y_test, U_test, categories, feature_names, granularity=10, DSExpansion=False, DSModification=False, compute_dev=False, report_verbose=False, clf=None, regul=0.00001, partitionMethod='median', penalty=None, fit_intercept=False, save_model=False, reload_model=False):\n",
    "    model_dump_file = path.join(GEOTEXT_HOME, 'model-' + DATASETS[DATASET_NUMBER - 1] + '-' + partitionMethod + '-' + str(BUCKET_SIZE) + '-' + str(regul) + '.pkl')\n",
    "    top_features_file = path.join(GEOTEXT_HOME, 'topfeatures-' + DATASETS[DATASET_NUMBER - 1] + '-' + partitionMethod + '-' + str(BUCKET_SIZE) + '-' + str(regul) + '.txt')\n",
    "    compute_dev = True\n",
    "\n",
    "    if clf == None:\n",
    "        # clf = LinearSVC(multi_class='ovr', class_weight='auto', C=1.0, loss='l2', penalty='l2', dual=False, tol=1e-3)\n",
    "        # clf = linear_model.LogisticRegression(C=1.0, penalty='l2')\n",
    "        # alpha = 0.000001\n",
    "        clf = SGDClassifier(loss='log', alpha=regul, penalty=penalty, l1_ratio=0.9, learning_rate='optimal', n_iter=10, shuffle=False, n_jobs=10, fit_intercept=fit_intercept)\n",
    "        # clf = LabelPropagation(kernel='rbf', gamma=50, n_neighbors=7, alpha=1, max_iter=30, tol=0.001)\n",
    "        # clf = LabelSpreading(kernel='rbf', gamma=20, n_neighbors=7, alpha=0.2, max_iter=30, tol=0.001)\n",
    "        # clf = ensemble.AdaBoostClassifier()\n",
    "        # clf = ensemble.RandomForestClassifier(n_jobs=10)\n",
    "        # clf = MultiTaskLasso()\n",
    "        # clf = ElasticNet()\n",
    "        # clf = linear_model.Lasso(alpha = 0.1)\n",
    "        \n",
    "        # clf = SGDClassifier(loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, class_weight, warm_start, rho, seed)\n",
    "        # clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
    "        # clf = RidgeClassifier(tol=1e-2, solver=\"auto\")\n",
    "        # clf = RidgeClassifier(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=1e-2, class_weight=None, solver=\"auto\")\n",
    "        # clf = SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n",
    "        # clf = Perceptron(n_iter=50)\n",
    "        # clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "        # clf = KNeighborsClassifier(n_neighbors=10)\n",
    "        # clf = NearestCentroid()\n",
    "        # clf = MultinomialNB(alpha=.01)\n",
    "    \n",
    "    clf_requires_dense = False\n",
    "    if clf_requires_dense:\n",
    "        X_train = X_train.toarray()\n",
    "        X_test = X_test.toarray()\n",
    "        X_dev = X_dev.toarray()\n",
    "    model_reloaded = False\n",
    "    if reload_model and path.exists(model_dump_file):\n",
    "        print('loading a trained model from %s' % (model_dump_file))\n",
    "        with open(model_dump_file, 'rb') as inf:\n",
    "            clf = pickle.load(inf)\n",
    "            model_reloaded = True\n",
    "        print(clf)\n",
    "    if not model_reloaded:\n",
    "        print('_' * 80)\n",
    "        print(\"Training: \")\n",
    "        print(clf)\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        train_time = time.time() - t0\n",
    "        print(\"train time: %0.3fs\" % train_time)\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            zero_count = (clf.coef_ == 0).sum()\n",
    "            total_param_count = clf.coef_.shape[0] * clf.coef_.shape[1]\n",
    "            zero_percent = int(100 * float(zero_count) / total_param_count)\n",
    "            print('%d percent sparse' % (zero_percent))\n",
    "            # if zero_percent > 50:\n",
    "                # print('sparsifying clf.coef_ to free memory')\n",
    "                # clf.sparsify()\n",
    "        if save_model:\n",
    "            print('dumpinng the model in %s' % (model_dump_file))\n",
    "            with open(model_dump_file, 'wb') as outf:\n",
    "                pickle.dump(clf, outf)\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        non_zero_parameters = csr_matrix(clf.coef_).nnz\n",
    "    else:\n",
    "        non_zero_parameters = 0\n",
    "\n",
    "    report_verbose = True\n",
    "    if compute_dev:\n",
    "        devPreds = clf.predict(X_dev)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            devProbs = clf.predict_proba(X_dev)\n",
    "        else:\n",
    "            devProbs = Nonel\n",
    "\n",
    "    save_vocabulary = False\n",
    "    if save_vocabulary:\n",
    "        rows, cols = np.nonzero(clf.coef_)\n",
    "        del rows\n",
    "        vocab = feature_names[cols]\n",
    "        vocab_file = path.join(GEOTEXT_HOME, 'vocab.pkl')\n",
    "        with open(vocab_file, 'wb') as outf:\n",
    "            pickle.dump(vocab, outf)\n",
    "    abod = False\n",
    "    print \"predicting test labels\"  \n",
    "    t0 = time.time()\n",
    "    preds = clf.predict(X_test)\n",
    "    # scores = clf.decision_function(X_test)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        testProbs = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        testProbs = None\n",
    "    probs = None\n",
    "    # print preds.shape\n",
    "    test_time = time.time() - t0\n",
    "    print(\"test time: %0.3fs\" % test_time)\n",
    "    \n",
    "\n",
    "    report_verbose = False\n",
    "    if report_verbose:\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "            print(\"density: %f\" % density(clf.coef_))\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            with codecs.open(top_features_file, 'w', encoding='utf-8') as outf:\n",
    "                for i, category in enumerate(categories):\n",
    "                    top10 = np.argsort(clf.coef_[i])[-50:]\n",
    "                    # print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "                    outf.write(category + \": \" + \" \".join(feature_names[top10]) + '\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    print \"test results\"\n",
    "    meanTest, medianTest, acc_at_161_test = loss(preds, U_test, save=True)\n",
    "    meanDev = -1\n",
    "    medianDev = -1\n",
    "    # **************AUSKOMMENTIERT******************\n",
    "    if compute_dev:\n",
    "        print \"development results\"\n",
    "        meanDev, medianDev, acc_at_161_dev = loss(devPreds, U_dev,save=False, save_results_bucket_size=True, results_key='lr')\n",
    "    dump_preds = True\n",
    "    if dump_preds:\n",
    "        result_dump_file = \"C:/Users/Dennis/Desktop/geolocation/dump/dump.pkl\"\n",
    "        print \"dumping preds (preds, devPreds, U_test, U_dev, testProbs, devProbs) in \" + result_dump_file\n",
    "        #Output here\n",
    "    \n",
    "    dumping_preds.append(preds)\n",
    "    dumping_preds.append(devPreds)\n",
    "    dumping_preds.append(U_test)\n",
    "    dumping_preds.append(U_dev)\n",
    "    dumping_preds.append(testProbs)\n",
    "    dumping_preds.append(dev_Probs)\n",
    "    #with open(result_dump_file, 'wb') as outf:\n",
    "    #        pickle.dump((preds, devPreds, U_test, U_dev, testProbs, devProbs), outf)\n",
    "    # evaluate(preds,U_test, categories, None)\n",
    "    # abod(probs, preds, U_test)\n",
    "    return preds, probs, U_test, meanTest, medianTest, acc_at_161_test, meanDev, medianDev, acc_at_161_dev, non_zero_parameters\n",
    "   \n",
    "# classify()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize(partitionMethod, granularity, write=False, readText=True, reload_init=False, regression=False):    \n",
    "\n",
    "    reload_file = path.join(GEOTEXT_HOME + '/init_' + DATASETS[DATASET_NUMBER - 1] + '_' + partitionMethod + '_' + str(BUCKET_SIZE) + '.pkl')\n",
    "\n",
    "    logging.info('reading (user_info.) train, dev and test file and building trainUsers, devUsers and testUsers with their locations')\n",
    "    users(trainfile, 'train', write, readText=readText)\n",
    "    users(devfile, 'dev', write, readText=readText)\n",
    "    users(testfile, 'test', write, readText=readText)\n",
    "    logging.info(\"the number of train\" + \" users is \" + str(len(trainUsers)))\n",
    "    logging.info(\"the number of test\" + \" users is \" + str(len(testUsers)))\n",
    "    logging.info(\"the number of dev\" + \" users is \" + str(len(devUsers)))\n",
    "\n",
    "    if not regression:\n",
    "        create_directories(granularity, partitionMethod, write)  \n",
    "        write_init_info = False\n",
    "        if write_init_info:\n",
    "            print('writing init info in %s' % (reload_file))\n",
    "            with open(reload_file, 'wb') as outf:\n",
    "                pickle.dump((classLatMean, classLonMedian, classLatMedian, classLonMean, userLocation, categories, trainUsers, trainClasses, testUsers, testClasses, devUsers, devClasses), outf)\n",
    "    else:\n",
    "        logging.info('Not discretising locations as regression option is on!')\n",
    "    logging.info(\"initialization finished\")\n",
    "\n",
    "\n",
    "def factorize(X, transformees, factorizer=None):\n",
    "    if factorizer is None:\n",
    "        factorizer = DictionaryLearning(n_components=100, alpha=1, max_iter=100, tol=1e-8, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=30, code_init=None, dict_init=None, verbose=True, split_sign=None, random_state=None)\n",
    "    else:\n",
    "        print factorizer\n",
    "    # dic_learner = DictionaryLearning(n_components=100, alpha=1, max_iter=100, tol=1e-8, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=30, code_init=None, dict_init=None, verbose=True, split_sign=None, random_state=None)\n",
    "    # dic_learner = MiniBatchDictionaryLearning(n_components=20, alpha=1, n_iter=100, fit_algorithm='lars', n_jobs=30, batch_size=1000, shuffle=True, dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, verbose=True, split_sign=True, random_state=None)\n",
    "    # dic_learner = PCA(n_components=500)\n",
    "    \n",
    "    \n",
    "    if sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "    factorizer.fit(X)\n",
    "    results = []\n",
    "    for feature_matrix in transformees:\n",
    "        if sparse.issparse(feature_matrix):\n",
    "            feature_matrix = feature_matrix.toarray()\n",
    "        feature_matrix = factorizer.transform(feature_matrix)\n",
    "        results.append(feature_matrix)\n",
    "    return results\n",
    "\n",
    "def asclassification(granularity, partitionMethod, use_mention_dictionary=False, use_sparse_code=False, penalty=None, fit_intercept=False, norm=None, binary=False, sublinear=False, factorizer=None, read_vocab=False, use_idf=True):\n",
    "\n",
    "    if read_vocab:\n",
    "        vocab_file = path.join(GEOTEXT_HOME, 'vocab.pkl')\n",
    "        with open(vocab_file, 'rb') as inf:\n",
    "            vocab = pickle.load(inf)\n",
    "            vocab = list(set(vocab))\n",
    "    else:\n",
    "        vocab = None\n",
    "    stops = 'english'\n",
    "    # partitionLocView(granularity=granularity, partitionMethod=partitionMethod)\n",
    "    X_train, Y_train, U_train, X_dev, Y_dev, U_dev, X_test, Y_test, U_test, categories, feature_names = feature_extractor2(norm=norm, use_mention_dictionary=use_mention_dictionary, min_df=10, max_df=0.2, stop_words=stops, binary=binary, sublinear_tf=sublinear, vocab=vocab, use_idf=use_idf)    \n",
    "    if use_sparse_code:\n",
    "        print(\"using a matrix factorization technique to learn a better representation of users...\")\n",
    "        sparse_coded_dump = path.join(GEOTEXT_HOME, 'sparse_coded.pkl')\n",
    "        if os.path.exists(sparse_coded_dump) and DATASET_NUMBER != 1:\n",
    "            with open(sparse_coded_dump, 'rb') as inf:\n",
    "                X_train, X_dev, X_test = pickle.load(inf)\n",
    "        else:\n",
    "            X_train, X_dev, X_test = factorize(X_train, transformees=[X_train, X_dev, X_test], factorizer=factorizer)\n",
    "            save_sparse = False\n",
    "            if save_sparse:\n",
    "                with open(sparse_coded_dump, 'wb') as inf:\n",
    "                    pickle.dump((X_train, X_dev, X_test), inf)\n",
    "    \n",
    "    best_dev_acc = -1\n",
    "    best_dev_mean = -1\n",
    "    best_dev_median = -1\n",
    "    best_regul = -1\n",
    "    best_test_preds = None\n",
    "    regul_acc = {}\n",
    "    regul_nonzero = {}\n",
    "    \n",
    "    \n",
    "    if DATASET_NUMBER == 1:\n",
    "        reguls_coefs = [5e-8, 1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 4e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-2, 1e-2]\n",
    "    elif DATASET_NUMBER == 2:\n",
    "        reguls_coefs = [1e-8, 5e-8, 1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5]\n",
    "        reguls_coefs = [2 ** a for a in xrange(-27, -3, 2)]\n",
    "        if BUCKET_SIZE <= 1024:\n",
    "            reguls_coefs = [2 ** -21]\n",
    "        else:\n",
    "            reguls_coefs = [2 ** -19]\n",
    "    elif DATASET_NUMBER == 3:\n",
    "        reguls_coefs = [1e-8, 5e-8, 1e-7, 5e-7, 1e-6]\n",
    "    if penalty == 'none':\n",
    "        reguls_coefs = [1e-200]\n",
    "    # for regul in reguls_coefs:\n",
    "    for regul in [reguls[DATASET_NUMBER - 1]]:\n",
    "        preds, probs, U_test, meanTest, medianTest, acc_at_161_test, meanDev, medianDev, acc_at_161_dev, non_zero_parameters = classify(X_train, Y_train, U_train, X_dev, Y_dev, U_dev, X_test, Y_test, U_test, categories, feature_names, granularity=granularity, regul=regul, partitionMethod=partitionMethod, penalty=penalty, fit_intercept=fit_intercept, reload_model=False)\n",
    "        regul_acc[regul] = acc_at_161_dev\n",
    "        regul_nonzero[regul] = non_zero_parameters\n",
    "        if acc_at_161_dev > best_dev_acc:\n",
    "            best_dev_acc = acc_at_161_dev\n",
    "            best_regul = regul\n",
    "            best_test_preds = preds\n",
    "            best_dev_mean = meanDev\n",
    "            best_dev_median = medianDev\n",
    "            \n",
    "    print('The best regul_coef is %e %f' % (best_regul, best_dev_acc))\n",
    "    t_mean, t_median, t_acc = loss(best_test_preds, U_test)\n",
    "    \n",
    "    return t_mean, t_median, t_acc, best_dev_mean, best_dev_median, best_dev_acc\n",
    "    # return preds, probs, U_test, meanTest, medianTest, acc_at_161_test, meanDev, medianDev, acc_at_161_dev\n",
    "\n",
    "\n",
    "def locationStr2Float(locationStr):\n",
    "    latlon = locationStr.split(',')\n",
    "    lat = float(latlon[0])\n",
    "    lon = float(latlon[1])\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_mentions(k=0, addTest=False, addDev=False):\n",
    "    if addTest and addDev:\n",
    "        print \"addTest and addDev can not be True in the same time\"\n",
    "        sys.exit(0)\n",
    "    print \"extracting mention information from text\"\n",
    "    global mentions\n",
    "    # if it is there load it and return\n",
    "    mention_file_address = path.join(GEOTEXT_HOME, 'mentions.pkl')\n",
    "    if addDev:\n",
    "        mention_file_address = mention_file_address + '.dev'\n",
    "    RELOAD_MENTIONS = True\n",
    "    if RELOAD_MENTIONS:\n",
    "        if os.path.exists(mention_file_address):\n",
    "            print \"reading mentions from pickle\"\n",
    "            with open(mention_file_address, 'rb') as inf:\n",
    "                mentions = pickle.load(inf)\n",
    "                return\n",
    "    text = ''\n",
    "    # for user in trainUsers:\n",
    "    #    text += userText[user].lower()\n",
    "    if addTest:\n",
    "        text = ' '.join(trainText.values() + testText.values())\n",
    "    if addDev:\n",
    "        text = ' '.join(trainText.values() + devText.values())\n",
    "    if not addTest and not addDev:\n",
    "        text = ' '.join(trainText.values())\n",
    "    # text = text.lower()\n",
    "    '''\n",
    "    if data_encoding in ['utf-8']:\n",
    "        text = strip_accents_unicode(text)\n",
    "    elif data_encoding in ['latin', 'latin1']:\n",
    "        text = strip_accents_ascii(text)\n",
    "    '''\n",
    "    token_pattern = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    print \"finding mentions\"\n",
    "    mentionsList = [word.lower() for word in token_pattern.findall(text)]\n",
    "    print \"building the counter\"\n",
    "    mentionsDic = Counter(mentionsList)\n",
    "    print \"frequency thresholding\"\n",
    "    if k > 0:\n",
    "        mentions = [word for word in mentionsDic if mentionsDic[word] > k]\n",
    "    else:\n",
    "        mentions = mentionsDic.keys()\n",
    "\n",
    "    with open(mention_file_address, 'wb') as outf:\n",
    "        print \"writng mentions to pickle\"\n",
    "        pickle.dump(mentions, outf)\n",
    "\n",
    "def prepare_adsorption_data_collapsed(DEVELOPMENT=False, text_prior='none', CELEBRITY_THRESHOLD=100000, build_networkx_graph=False, DIRECT_GRAPH_WEIGHTED=False, partitionMethod='median'):\n",
    "    MULTI_LABEL = False\n",
    "\n",
    "    dongle_nodes = None\n",
    "    dongle_preds = None\n",
    "    dongle_probs = None\n",
    "    U_train = [u for u in sorted(trainUsers)]\n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "    text_str = ''\n",
    "    if text_prior != 'none':\n",
    "        text_str = '.' + text_prior\n",
    "    weighted_str = ''\n",
    "    if DIRECT_GRAPH_WEIGHTED:\n",
    "        weighted_str = '.weighted'\n",
    "    \n",
    "    celebrityStr = str(CELEBRITY_THRESHOLD)\n",
    "\n",
    "    \n",
    "    # split a training set and a test set\n",
    "    Y_train = np.asarray([trainClasses[u] for u in U_train])\n",
    "    # Y_test = np.asarray([testClasses[u] for u in U_test])\n",
    "    # Y_dev = np.asarray([devClasses[u] for u in U_dev])\n",
    "\n",
    "    \n",
    "    if DEVELOPMENT:\n",
    "        devStr = '.dev'\n",
    "    else:\n",
    "        devStr = ''\n",
    "\n",
    "    \n",
    "    if text_prior != 'none':\n",
    "        logging.info(\"tex prior is \" + text_prior)\n",
    "        # read users and predictions\n",
    "        result_dump_file = path.join(GEOTEXT_HOME, 'results-' + DATASETS[DATASET_NUMBER - 1] + '-' + partitionMethod + '-' + str(BUCKET_SIZE) + '.pkl')\n",
    "        t_preds, d_preds, t_users, d_users, t_probs, d_probs = None, None, None, None, None, None\n",
    "        logging.info(\"reading the text learner results from \" + result_dump_file)\n",
    "        with open(result_dump_file, 'rb') as inf:\n",
    "            t_preds, d_preds, t_users, d_users, t_probs, d_probs = pickle.load(inf)\n",
    "\n",
    "        if DEVELOPMENT:\n",
    "            dongle_nodes = d_users\n",
    "            dongle_preds = d_preds\n",
    "            dongle_probs = d_probs\n",
    "            logging.info(\"text dev results:\")\n",
    "            loss(d_preds, d_users)\n",
    "        else:\n",
    "            dongle_nodes = t_users\n",
    "            dongle_preds = t_preds\n",
    "            dongle_probs = t_probs\n",
    "            logging.info(\"text test results:\")\n",
    "            loss(t_preds, t_users)\n",
    "              \n",
    "\n",
    "    id_user_file = path.join(GEOTEXT_HOME, 'id_user_' + partitionMethod + '_' + str(BUCKET_SIZE) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"writing id_user in \" + id_user_file)\n",
    "    with codecs.open(id_user_file, 'w', 'ascii') as outf:\n",
    "        for i in range(0, len(U_train)):\n",
    "            outf.write(str(i) + '\\t' + U_train[i] + '\\n')\n",
    "        if DEVELOPMENT:\n",
    "            for i in range(0, len(U_dev)):\n",
    "                outf.write(str(i + len(U_train)) + '\\t' + U_dev[i] + '\\n')\n",
    "        else:\n",
    "            for i in range(0, len(U_test)):\n",
    "                outf.write(str(i + len(U_train)) + '\\t' + U_test[i] + '\\n')\n",
    "\n",
    "    seed_file = path.join(GEOTEXT_HOME, 'seeds_' + partitionMethod + '_' + str(BUCKET_SIZE) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"writing seeds in \" + seed_file)\n",
    "    with codecs.open(seed_file, 'w', 'ascii') as outf:\n",
    "        for i in range(0, len(U_train)):\n",
    "            outf.write(str(i) + '\\t' + str(Y_train[i]) + '\\t' + '1.0' + '\\n')\n",
    "\n",
    "        if text_prior == 'dongle':\n",
    "            for i in range(0, len(dongle_nodes)):\n",
    "                # w = np.max(dongle_probs[i])\n",
    "                w = 1\n",
    "                outf.write(str(i + len(U_train)) + '.T' + '\\t' + str(dongle_preds[i]) + '\\t' + str(w) + '\\n')\n",
    "        elif text_prior == 'direct':\n",
    "            w = np.max(dongle_probs[i])\n",
    "            outf.write(str(i + len(U_train)) + '\\t' + str(dongle_preds[i]) + '\\t' + str(w) + '\\n')\n",
    "        elif text_prior == 'backoff':\n",
    "            pass \n",
    "\n",
    "    doubles = 0\n",
    "    double_nodes = []\n",
    "    trainIdx = range(len(U_train))\n",
    "    trainUsersLowerDic = dict(zip(U_train, trainIdx))\n",
    "    if DEVELOPMENT:\n",
    "        devStr = '.dev'\n",
    "        for i in range(0, len(U_dev)):\n",
    "            u = U_dev[i]\n",
    "            if u in trainUsersLowerDic:\n",
    "                U_dev[i] = u + '_double00'\n",
    "                double_nodes.append(u)\n",
    "                doubles += 1\n",
    "        u_unknown = U_dev\n",
    "        u_text_unknown = devText\n",
    "    else:\n",
    "        for i in range(0, len(U_test)):\n",
    "            u = U_test[i]\n",
    "            if u in trainUsersLowerDic:\n",
    "                U_test[i] = u + '_double00'\n",
    "                double_nodes.append(u)\n",
    "                doubles += 1\n",
    "        u_text_unknown = testText\n",
    "        u_unknown = U_test\n",
    "    if text_prior != 'none':\n",
    "        assert(len(u_unknown) == len(dongle_nodes)), 'the number of text/dev users is not equal to the number of text predictions.'\n",
    "    U_all = U_train + u_unknown \n",
    "\n",
    "    logging.info(\"The number of test users found in train users is \" + str(doubles))\n",
    "    logging.info(\"Double users: \" + str(double_nodes))\n",
    "    vocab_cnt = len(U_all)\n",
    "    idx = range(vocab_cnt)\n",
    "    node_id = dict(zip(U_all, idx))\n",
    "    # data and indices of a coo matrix to be populated\n",
    "    coordinates = Counter()\n",
    "    \n",
    "    # for node, id in node_id.iteritems():\n",
    "    #    node_lower_id[node.lower()] = id\n",
    "    assert (len(node_id) == len(U_train) + len(u_unknown)), 'number of unique users is not eq u_train + u_test'\n",
    "    logging.info(\"the number of nodes is \" + str(vocab_cnt))\n",
    "    logging.info(\"Adding the direct relationships...\")\n",
    "    token_pattern1 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern1 = re.compile(token_pattern1)\n",
    "    mention_users = defaultdict(Counter)\n",
    "    directly_connected_users = set()\n",
    "    l = len(trainText)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    for user, text in trainText.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        if i % tenpercent == 0:\n",
    "            logging.info(str(10 * i / tenpercent) + \"%\")\n",
    "        i += 1  \n",
    "        mentions = [u.lower() for u in token_pattern1.findall(text)] \n",
    "        mentionDic = Counter(mentions)\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            # check if mention is a user node\n",
    "            mention_id = node_id.get(mention, -1)\n",
    "            if mention_id not in [-1, user_id]:    \n",
    "                if user_id < mention_id:\n",
    "                    coordinates[(user_id, mention_id)] += freq\n",
    "                    directly_connected_users.add(user_id)\n",
    "                    directly_connected_users.add(mention_id)\n",
    "                elif mention_id < user_id:\n",
    "                    coordinates[(mention_id, user_id)] += freq\n",
    "                    directly_connected_users.add(user_id)\n",
    "                    directly_connected_users.add(mention_id)\n",
    "\n",
    "                \n",
    "            mention_users[mention][user_id] += freq\n",
    "    \n",
    "    \n",
    "    \n",
    "    logging.info(\"adding the eval graph\")\n",
    "    for user, text in u_text_unknown.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        mentions = [u.lower() for u in token_pattern1.findall(text)]\n",
    "        mentionDic = Counter(mentions)\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            mention_id = node_id.get(mention, -1)\n",
    "            if mention_id != -1:\n",
    "                if mention_id != user_id:\n",
    "                    if user_id < mention_id:\n",
    "                        coordinates[(user_id, mention_id)] += freq\n",
    "                        directly_connected_users.add(user_id)\n",
    "                        directly_connected_users.add(mention_id)\n",
    "                    elif mention_id < user_id:\n",
    "                        coordinates[(mention_id, user_id)] += freq\n",
    "                        directly_connected_users.add(user_id)\n",
    "                        directly_connected_users.add(mention_id)\n",
    "\n",
    "            mention_users[mention][user_id] += freq\n",
    "    \n",
    "    \n",
    "    \n",
    "    logging.info(\"Direct Relationships: \" + str(len(coordinates)))\n",
    "    logging.info(\"Directly related users: \" + str(len(directly_connected_users)) + \" percent: \" + str(100.0 * len(directly_connected_users) / len(U_all)))\n",
    "    logging.info(\"Adding the collapsed indirect relationships...\")\n",
    "\n",
    "    l = len(mention_users)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    celebrities_count = 0\n",
    "    for mention, user_ids in mention_users.iteritems():\n",
    "        if i % tenpercent == 0:\n",
    "            logging.info(str(10 * i / tenpercent) + \"%\")\n",
    "        i += 1  \n",
    "        if len(user_ids) > CELEBRITY_THRESHOLD:\n",
    "            celebrities_count += 1\n",
    "            continue\n",
    "        \n",
    "        for user_id1, freq1 in user_ids.iteritems():\n",
    "            for user_id2, freq2 in user_ids.iteritems():\n",
    "                if user_id1 < user_id2:\n",
    "                    coordinates[(user_id1, user_id2)] += (freq1 + freq2)\n",
    "\n",
    "    # free memory by deleting mention_users\n",
    "    del mention_users\n",
    "    logging.info(\"The number of celebrities is \" + str(celebrities_count) + \" .\")\n",
    "    logging.info(\"The number of edges is \" + str(len(coordinates)))\n",
    "    l = len(coordinates)\n",
    "    tenpercent = l / 10\n",
    "    input_graph_file = path.join(GEOTEXT_HOME, 'input_graph_' + partitionMethod + '_' + str(BUCKET_SIZE) + '_' + celebrityStr + devStr + text_str + weighted_str)\n",
    "    logging.info(\"writing the input_graph in \" + input_graph_file)\n",
    "    with codecs.open(input_graph_file, 'w', 'ascii', buffering=pow(2, 6) * pow(2, 20)) as outf:\n",
    "        i = 1\n",
    "        for nodes, w in coordinates.iteritems():\n",
    "            xindx, yindx = nodes\n",
    "            if not DIRECT_GRAPH_WEIGHTED:\n",
    "                w = 1.0\n",
    "            if i % tenpercent == 0:\n",
    "                logging.info(\"processing \" + str(10 * i / tenpercent) + \"%\")\n",
    "            i += 1\n",
    "            outf.write(str(xindx) + '\\t' + str(yindx) + '\\t' + str(w) + '\\n')\n",
    "            if build_networkx_graph:\n",
    "                mention_graph.add_edge(xindx, yindx, attr_dict={'w':w})\n",
    "        if text_prior == 'dongle':\n",
    "            for i in range(0, len(dongle_nodes)):\n",
    "                confidence = np.max(dongle_probs[i])\n",
    "                outf.write(str(i + len(U_train)) + '.T' + '\\t' + str(i + len(U_train)) + '\\t' + str(confidence) + '\\n')\n",
    "        elif text_prior == 'direct':\n",
    "            pass\n",
    "        elif text_prior == 'backoff':\n",
    "            pass\n",
    "    \n",
    "    u1s = [a for a, b in coordinates]\n",
    "    u2s = [b for a, b in coordinates]\n",
    "    us = set(u1s + u2s)\n",
    "    logging.info(\"the number of disconnected users is \" + str(len(U_all) - len(us)))\n",
    "    logging.info(\"the number of disconnected test users is \" + str(len(u_unknown) - len([a for a in us if a >= len(U_train)])))\n",
    "    output_file = path.join(GEOTEXT_HOME, 'label_prop_output_' + DATASETS[DATASET_NUMBER - 1] + '_' + partitionMethod + '_' + str(BUCKET_SIZE) + '_' + str(celeb_threshold) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"output file: \" + output_file)\n",
    "    # logging.info(str(disconnected_us))\n",
    "\n",
    "def prepare_adsorption_data_collapsed_networkx(DEVELOPMENT=False, text_prior='none', CELEBRITY_THRESHOLD=100000, build_networkx_graph=False, DIRECT_GRAPH_WEIGHTED=True, partitionMethod='median', postfix='.nx'):\n",
    "    global mentions\n",
    "\n",
    "    MULTI_LABEL = False\n",
    "\n",
    "    dongle_nodes = None\n",
    "    dongle_preds = None\n",
    "    dongle_probs = None\n",
    "    U_train = [u for u in sorted(trainUsers)]\n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "    text_str = ''\n",
    "    if text_prior != 'none':\n",
    "        text_str = '.' + text_prior\n",
    "    weighted_str = ''\n",
    "    if DIRECT_GRAPH_WEIGHTED:\n",
    "        weighted_str = '.weighted'\n",
    "    \n",
    "    celebrityStr = str(CELEBRITY_THRESHOLD)\n",
    "    mention_graph = nx.Graph()\n",
    "\n",
    "    Y_train = np.asarray([trainClasses[u] for u in U_train])\n",
    "\n",
    "    \n",
    "    if DEVELOPMENT:\n",
    "        devStr = '.dev'\n",
    "    else:\n",
    "        devStr = ''\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if text_prior != 'none':\n",
    "        logging.info(\"tex prior is \" + text_prior)\n",
    "        # read users and predictions\n",
    "        result_dump_file = path.join(GEOTEXT_HOME, 'results-' + DATASETS[DATASET_NUMBER - 1] + '-' + partitionMethod + '-' + str(BUCKET_SIZE) + '.pkl')\n",
    "        t_preds, d_preds, t_users, d_users, t_probs, d_probs = None, None, None, None, None, None\n",
    "        logging.info(\"reading the text learner results from \" + result_dump_file)\n",
    "        with open(result_dump_file, 'rb') as inf:\n",
    "            t_preds, d_preds, t_users, d_users, t_probs, d_probs = pickle.load(inf)\n",
    "\n",
    "        if DEVELOPMENT:\n",
    "            dongle_nodes = d_users\n",
    "            dongle_preds = d_preds\n",
    "            dongle_probs = d_probs\n",
    "            logging.info(\"text dev results:\")\n",
    "            loss(d_preds, d_users)\n",
    "        else:\n",
    "            dongle_nodes = t_users\n",
    "            dongle_preds = t_preds\n",
    "            dongle_probs = t_probs\n",
    "            logging.info(\"text test results:\")\n",
    "            loss(t_preds, t_users)\n",
    "              \n",
    "    id_user_file = path.join(GEOTEXT_HOME, 'id_user_' + partitionMethod + '_' + str(BUCKET_SIZE) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"writing id_user in \" + id_user_file)\n",
    "    with codecs.open(id_user_file, 'w', 'ascii') as outf:\n",
    "        for i in range(0, len(U_train)):\n",
    "            outf.write(str(i) + '\\t' + U_train[i] + '\\n')\n",
    "        if DEVELOPMENT:\n",
    "            for i in range(0, len(U_dev)):\n",
    "                outf.write(str(i + len(U_train)) + '\\t' + U_dev[i] + '\\n')\n",
    "        else:\n",
    "            for i in range(0, len(U_test)):\n",
    "                outf.write(str(i + len(U_train)) + '\\t' + U_test[i] + '\\n')\n",
    "\n",
    "    seed_file = path.join(GEOTEXT_HOME, 'seeds_' + partitionMethod + '_' + str(BUCKET_SIZE) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"writing seeds in \" + seed_file)\n",
    "    with codecs.open(seed_file, 'w', 'ascii') as outf:\n",
    "        for i in range(0, len(U_train)):\n",
    "            outf.write(str(i) + '\\t' + str(Y_train[i]) + '\\t' + '1.0' + '\\n')\n",
    "\n",
    "        if text_prior == 'dongle':\n",
    "            for i in range(0, len(dongle_nodes)):\n",
    "                # w = np.max(dongle_probs[i])\n",
    "                w = 1\n",
    "                outf.write(str(i + len(U_train)) + '.T' + '\\t' + str(dongle_preds[i]) + '\\t' + str(w) + '\\n')\n",
    "        elif text_prior == 'direct':\n",
    "            w = np.max(dongle_probs[i])\n",
    "            outf.write(str(i + len(U_train)) + '\\t' + str(dongle_preds[i]) + '\\t' + str(w) + '\\n')\n",
    "        elif text_prior == 'backoff':\n",
    "            pass \n",
    "\n",
    "    doubles = 0\n",
    "    double_nodes = []\n",
    "    trainIdx = range(len(U_train))\n",
    "    trainUsersLowerDic = dict(zip(U_train, trainIdx))\n",
    "    if DEVELOPMENT:\n",
    "        devStr = '.dev'\n",
    "        for i in range(0, len(U_dev)):\n",
    "            u = U_dev[i]\n",
    "            if u in trainUsersLowerDic:\n",
    "                U_dev[i] = u + '_double00'\n",
    "                double_nodes.append(u)\n",
    "                doubles += 1\n",
    "        u_unknown = U_dev\n",
    "        u_text_unknown = devText\n",
    "    else:\n",
    "        for i in range(0, len(U_test)):\n",
    "            u = U_test[i]\n",
    "            if u in trainUsersLowerDic:\n",
    "                U_test[i] = u + '_double00'\n",
    "                double_nodes.append(u)\n",
    "                doubles += 1\n",
    "        u_text_unknown = testText\n",
    "        u_unknown = U_test\n",
    "    if text_prior != 'none':\n",
    "        assert(len(u_unknown) == len(dongle_nodes)), 'the number of text/dev users is not equal to the number of text predictions.'\n",
    "    \n",
    "    \n",
    "    \n",
    "    U_all = U_train + u_unknown \n",
    "    logging.info(\"The number of test users found in train users is \" + str(doubles))\n",
    "    logging.info(\"Double users: \" + str(double_nodes))\n",
    "    vocab_cnt = len(U_all)\n",
    "    idx = range(vocab_cnt)\n",
    "    node_id = dict(zip(U_all, idx))\n",
    "    idx = list(idx)\n",
    "    mention_graph.add_nodes_from(idx)\n",
    "    # for node, id in node_id.iteritems():\n",
    "    #    node_lower_id[node.lower()] = id\n",
    "    assert (len(node_id) == len(U_train) + len(u_unknown)), 'number of unique users is not eq u_train + u_test'\n",
    "    logging.info(\"the number of nodes is \" + str(vocab_cnt))\n",
    "    logging.info(\"Adding the direct relationships...\")\n",
    "    print \"building the direct graph\"\n",
    "    token_pattern1 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern1 = re.compile(token_pattern1)\n",
    "    l = len(trainText)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    # add train and test users to the graph\n",
    "    mention_graph.add_nodes_from(node_id.values())\n",
    "    for user, text in trainText.iteritems():\n",
    "        user_id = node_id[user]    \n",
    "        if i % tenpercent == 0:\n",
    "            print str(10 * i / tenpercent) + \"%\"\n",
    "        i += 1  \n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id] \n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not DIRECT_GRAPH_WEIGHTED:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)   \n",
    "       \n",
    "    print \"adding the eval graph\"\n",
    "    for user, text in u_text_unknown.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id]\n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not DIRECT_GRAPH_WEIGHTED:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)  \n",
    "    \n",
    "    \n",
    "    celebrities = []\n",
    "    remove_celebrities = True\n",
    "    if remove_celebrities:\n",
    "        nodes = mention_graph.nodes_iter()\n",
    "        for node in nodes:\n",
    "            nbrs = mention_graph.neighbors(node)\n",
    "            if len(nbrs) > CELEBRITY_THRESHOLD:\n",
    "                celebrities.append(node)\n",
    "        celebrities = [c for c in celebrities if c not in idx]\n",
    "        logging.info(\"found %d celebrities with celebrity threshold %d\" % (len(celebrities), CELEBRITY_THRESHOLD))\n",
    "        for celebrity in celebrities:\n",
    "                mention_graph.remove_node(celebrity)\n",
    "    \n",
    "    project_to_main_users = True\n",
    "    if project_to_main_users:\n",
    "        # mention_graph = bipartite.overlap_weighted_projected_graph(mention_graph, main_users, jaccard=False)\n",
    "        mention_graph = collaboration_weighted_projected_graph(mention_graph, idx, weight_str=None, degree_power=1, caller='mad')\n",
    "        # mention_graph = collaboration_weighted_projected_graph(mention_graph, main_users, weight_str='weight')\n",
    "        # mention_graph = bipartite.projected_graph(mention_graph, main_users)\n",
    "    connected_nodes = set()\n",
    "    l = mention_graph.number_of_edges()\n",
    "    logging.info(\"The number of edges is \" + str(l))\n",
    "    tenpercent = l / 10\n",
    "    input_graph_file = path.join(GEOTEXT_HOME, 'input_graph_' + partitionMethod + '_' + str(BUCKET_SIZE) + '_' + celebrityStr + devStr + text_str + weighted_str + postfix)\n",
    "    logging.info(\"writing the input_graph in \" + input_graph_file)\n",
    "    with codecs.open(input_graph_file, 'w', 'ascii', buffering=pow(2, 6) * pow(2, 20)) as outf:\n",
    "        i = 1\n",
    "        for edge in mention_graph.edges_iter(nbunch=None, data=True):\n",
    "            u, v , data = edge\n",
    "            connected_nodes.add(u)\n",
    "            connected_nodes.add(v)\n",
    "            w = data['weight']\n",
    "            if not DIRECT_GRAPH_WEIGHTED:\n",
    "                w = 1.0\n",
    "            if i % tenpercent == 0:\n",
    "                logging.info(\"processing \" + str(10 * i / tenpercent) + \"%\")\n",
    "            i += 1\n",
    "            outf.write(str(u) + '\\t' + str(v) + '\\t' + str(w) + '\\n')\n",
    "        if text_prior == 'dongle':\n",
    "            for i in range(0, len(dongle_nodes)):\n",
    "                confidence = np.max(dongle_probs[i])\n",
    "                outf.write(str(i + len(U_train)) + '.T' + '\\t' + str(i + len(U_train)) + '\\t' + str(confidence) + '\\n')\n",
    "        elif text_prior == 'direct':\n",
    "            pass\n",
    "        elif text_prior == 'backoff':\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    us = [a for a in idx if a in connected_nodes]\n",
    "    logging.info(\"the number of disconnected users is \" + str(len(U_all) - len(us)))\n",
    "    logging.info(\"the number of disconnected test users is \" + str(len(u_unknown) - len([a for a in us if a >= len(U_train)])))\n",
    "    output_file = path.join(GEOTEXT_HOME, 'label_prop_output_' + DATASETS[DATASET_NUMBER - 1] + '_' + partitionMethod + '_' + str(BUCKET_SIZE) + '_' + str(celeb_threshold) + devStr + text_str + weighted_str)\n",
    "    logging.info(\"output file: \" + output_file)\n",
    "    # logging.info(str(disconnected_us))\n",
    "    \n",
    "\n",
    "\n",
    "def ideal_network_errors():\n",
    "    graph_file_address = path.join(GEOTEXT_HOME, 'direct_graph')\n",
    "    if os.path.exists(graph_file_address):\n",
    "        print \"reading netgraph from pickle\"\n",
    "        with open(graph_file_address, 'rb') as inf:\n",
    "            netgraph, trainUsers, testUsers = pickle.load(inf)\n",
    "    ideal_distances = []\n",
    "    acc161 = 0\n",
    "    tenpercent = len(testUsers) / 10\n",
    "    i = 0\n",
    "    for utest, uloc in testUsers.iteritems():\n",
    "        i += 1\n",
    "        if i % tenpercent == 0:\n",
    "            print str(100 * i / len(testUsers))\n",
    "        lat1, lon1 = locationStr2Float(uloc)\n",
    "        dists = []\n",
    "        for utrain, utrainloc in trainUsers.iteritems():\n",
    "            lat2, lon2 = locationStr2Float(utrainloc)\n",
    "            d = distance(lat1, lon1, lat2, lon2)\n",
    "            dists.append(distance(lat1, lon1, lat2, lon2))\n",
    "            if d < 1:\n",
    "                break\n",
    "        minDist = min(dists)\n",
    "        if minDist < 161:\n",
    "            acc161 += 1\n",
    "        ideal_distances.append(minDist)\n",
    "    print \"distance number\" + str(len(ideal_distances))\n",
    "    print \"mean \" + str(np.mean(ideal_distances))\n",
    "    print \"median \" + str(np.median(ideal_distances))\n",
    "    print \"Acc @ 161 \" + str((acc161 + 0.0) / len(ideal_distances))\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "def LP(weighted=True, prior='none', normalize_edge=False, remove_celebrities=False, dev=False, node_order='l2h', remove_mentions_with_degree_one=True):\n",
    "    '''\n",
    "    Run label propagation over real-valued coordinates of the @-mention graph.\n",
    "    The coordinates of the training users are kept unchanged and the coordinates\n",
    "    of other users are updated to the median of their neighbours.\n",
    "    '''\n",
    "    if dev:\n",
    "        evalText = devText\n",
    "        evalUsers = devUsers\n",
    "    else:\n",
    "        evalText = testText\n",
    "        evalUsers = testUsers\n",
    "    save_gr = True\n",
    "    verbose = False\n",
    "    mention_graph = nx.Graph()\n",
    "    graph_file_address = path.join(GEOTEXT_HOME, 'direct_graph.graphml')\n",
    "    U_all = trainUsers.keys() + evalUsers.keys()\n",
    "    assert len(U_all) == len(trainUsers) + len(evalUsers), \"duplicate user problem\"\n",
    "    idx = range(len(U_all))\n",
    "    node_id = dict(zip(U_all, idx))\n",
    "    \n",
    "    print('weighted=%s and prior=%s' % (weighted, prior))\n",
    "    print \"building the direct graph\"\n",
    "    token_pattern1 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern1 = re.compile(token_pattern1)\n",
    "    token_pattern2 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))#([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern2 = re.compile(token_pattern2)\n",
    "    netgraph = {}\n",
    "    l = len(trainText)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    # add train and test users to the graph\n",
    "    mention_graph.add_nodes_from([(u, {'train':True, 'loc':l}) for u, l in trainUsers.iteritems()])\n",
    "    mention_graph.add_nodes_from([(u, {'test':True, 'loc':l}) for u, l in evalUsers.iteritems()])\n",
    "    for user, text in trainText.iteritems():    \n",
    "        if i % tenpercent == 0:\n",
    "            print str(10 * i / tenpercent) + \"%\"\n",
    "        i += 1  \n",
    "        mentions = [u.lower() for u in token_pattern1.findall(text)] \n",
    "        mentions = [m for m in mentions if m != user]\n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user, mention):\n",
    "                mention_graph[user][mention]['weight'] += freq\n",
    "            else:\n",
    "                mention_graph.add_edge(user, mention, weight=freq)   \n",
    "        \n",
    "    print \"adding the eval graph\"\n",
    "    for user, text in evalText.iteritems():\n",
    "        mentions = [u.lower() for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user]\n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user, mention):\n",
    "                mention_graph[user][mention]['weight'] += freq\n",
    "            else:\n",
    "                mention_graph.add_edge(user, mention, weight=freq)  \n",
    "    \n",
    "\n",
    "    lp_graph = mention_graph          \n",
    "    trainUsersLower = {}\n",
    "    evalUsersLower = {}\n",
    "    trainLats = []\n",
    "    trainLons = []\n",
    "    node_location = {}\n",
    "    dongle_nodes = []\n",
    "    text_preds = {}\n",
    "    \n",
    "    dongle = False\n",
    "    backoff = False\n",
    "    text_direct = False\n",
    "    if prior != 'none':\n",
    "        if prior == 'backoff':\n",
    "            backoff = True\n",
    "        elif prior == 'prior':\n",
    "            text_direct = True\n",
    "        elif prior == 'dongle':\n",
    "            dongle = True\n",
    "        \n",
    "        prior_file_path=\"\"\n",
    "        \n",
    "        #prior_file_path is the path to the predictions for GeoLoc Text\n",
    "        \n",
    "        # Uncomment -> For our own training set\n",
    "        #prior_file_path = \"C:/Users/dennis/Desktop/geolocation/outputTextClass/tweetCov/geoLocLPClass.pkl\"\n",
    "        \n",
    "        #For geolocation training set \n",
    "        #prior_file_path= \"C:/Users/dennis/Desktop/geolocation/outputTextClass/preds0.pkl\"\n",
    "\n",
    "        multiple_pkl=True\n",
    "        \n",
    "        print \"reading prior text-based locations from \" + prior_file_path\n",
    "        if os.path.exists(prior_file_path):\n",
    "            \n",
    "            if multiple_pkl==False:\n",
    "                with open(prior_file_path, 'rb') as inf:\n",
    "                    preds = pickle.load(inf)\n",
    "\n",
    "                    if dongle:\n",
    "                        mention_graph.add_nodes_from([u + '.dongle' for u in U_test])\n",
    "                        \n",
    "                    user_index = 0\n",
    "                    for user, pred in preds.items():\n",
    "                        if backoff:\n",
    "                            text_preds[user] = pred\n",
    "                        #if dongle:\n",
    "                        #    dongle_node = user + '.dongle'\n",
    "                        #    w = test_confidences[user_index]\n",
    "                        #    mention_graph.add_edge(dongle_node, user, weight=w)\n",
    "                        #    node_location[dongle_node] = (lat, lon)\n",
    "                        #    dongle_nodes.append(dongle_node)\n",
    "                        #elif text_direct:\n",
    "                        #    node_location[user] = (lat, lon)\n",
    "                        user_index += 1                     \n",
    "            else:\n",
    "                file1=\"C:/Users/dennis/Desktop/geolocation/outputTextClass/preds0.pkl\"\n",
    "                file2=\"C:/Users/dennis/Desktop/geolocation/outputTextClass/preds1.pkl\"\n",
    "                file3=\"C:/Users/dennis/Desktop/geolocation/outputTextClass/preds2.pkl\"\n",
    "                pickle_dict1 = pickle.load(file1)\n",
    "                pickle_dict2 = pickle.load(file2)\n",
    "                pickle_dict3 = pickle.load(file3)\n",
    "                preds = pickle_dict1\n",
    "                preds.update(pickle_dict2)\n",
    "                preds.update(pickle_dict3)\n",
    "                \n",
    "                if dongle:\n",
    "                    mention_graph.add_nodes_from([u + '.dongle' for u in U_test])\n",
    "                user_index = 0\n",
    "                for user, pred in preds.items():\n",
    "\n",
    "                    if backoff:\n",
    "                        text_preds[user] = pred\n",
    "                    if dongle:\n",
    "                        dongle_node = user + '.dongle'\n",
    "                        w = test_confidences[user_index]\n",
    "                        mention_graph.add_edge(dongle_node, user, weight=w)\n",
    "                        node_location[dongle_node] = (lat, lon)\n",
    "                        dongle_nodes.append(dongle_node)\n",
    "                    elif text_direct:\n",
    "                        node_location[user] = (lat, lon)\n",
    "                    user_index += 1      \n",
    "        else:\n",
    "            for x in range(3):\n",
    "                prior_file_path=\"C:/Users/dennis/Desktop/geolocation/outputTextClass/preds\"+str(x)+\".pkl\"\n",
    "                with open(prior_file_path, 'rb') as inf:\n",
    "                    preds = pickle.load(inf)\n",
    "\n",
    "                    if dongle:\n",
    "                        mention_graph.add_nodes_from([u + '.dongle' for u in U_test])\n",
    "                    user_index = 0   \n",
    "                    for user, pred in preds.items():\n",
    "\n",
    "                        if backoff:\n",
    "                            text_preds[user] = pred\n",
    "                        if dongle:\n",
    "                            dongle_node = user + '.dongle'\n",
    "                            w = test_confidences[user_index]\n",
    "                            mention_graph.add_edge(dongle_node, user, weight=w)\n",
    "                            node_location[dongle_node] = (lat, lon)\n",
    "                            dongle_nodes.append(dongle_node)\n",
    "                        elif text_direct:\n",
    "                            node_location[user] = (lat, lon)\n",
    "                        user_index += 1\n",
    "    for user, loc in trainUsers.iteritems():\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        trainLats.append(lat)\n",
    "        trainLons.append(lon)\n",
    "        trainUsersLower[user] = (lat, lon)\n",
    "        node_location[user] = (lat, lon)\n",
    "        \n",
    "    for user, loc in evalUsers.iteritems():\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        evalUsersLower[user] = (lat, lon)\n",
    "    \n",
    "    lp_graph = evalUsersLower\n",
    "    print \"the number of train nodes is \" + str(len(trainUsers))\n",
    "    print \"the number of test nodes is \" + str(len(evalUsers))\n",
    "    medianLat = np.median(trainLats)\n",
    "    medianLon = np.median(trainLons)\n",
    "\n",
    "    # remove celebrities from the graph\n",
    "    \n",
    "    remove_betweeners = False\n",
    "    if remove_betweeners:\n",
    "        print(\"computing betweenness centrality of all nodes, takes a long time, sorry!\")\n",
    "        scores = nx.betweenness_centrality(mention_graph, weight='weight')\n",
    "        i = 0\n",
    "        percent_5 = len(scores) / 20 \n",
    "        for w in sorted(scores, key=scores.get, reverse=True):\n",
    "            i += 1\n",
    "            if i < percent_5:\n",
    "                mention_graph.remove_node(w)\n",
    "    \n",
    "    \n",
    "    celebrity_threshold = celeb_threshold\n",
    "    celebrities = []\n",
    "    if remove_celebrities:\n",
    "        nodes = mention_graph.nodes_iter()\n",
    "        for node in nodes:\n",
    "            nbrs = mention_graph.neighbors(node)\n",
    "            if len(nbrs) > celebrity_threshold:\n",
    "                if node not in evalUsersLower and node not in trainUsersLower:\n",
    "                    celebrities.append(node)\n",
    "        print(\"found %d celebrities with celebrity threshold %d\" % (len(celebrities), celebrity_threshold))\n",
    "        for celebrity in celebrities:\n",
    "            mention_graph.remove_node(celebrity)\n",
    " \n",
    "    if remove_mentions_with_degree_one:\n",
    "        mention_nodes = set(mention_graph.nodes()) - set(node_id.values())\n",
    "        mention_degree = mention_graph.degree(nbunch=mention_nodes, weight=None)\n",
    "        one_degree_non_target = [node for node, degree in mention_degree.iteritems() if degree < 2]\n",
    "        logging.info('found ' + str(len(one_degree_non_target)) + ' mentions with degree 1 in the graph.')\n",
    "        for node in one_degree_non_target:\n",
    "            mention_graph.remove_node(node)\n",
    "    print \"finding unlocated nodes\"\n",
    "    if node_order:\n",
    "        node_degree = mention_graph.degree()\n",
    "        # sort node_degree by value\n",
    "        if node_order == 'h2l':\n",
    "            reverse_order = True\n",
    "        else:\n",
    "            reverse_order = False\n",
    "        nodes = sorted(node_degree, key=node_degree.get, reverse=reverse_order)\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes)\n",
    "        # nodes_unknown = [node for node in mention_graph.nodes() if node not in trainUsersLower and node not in dongle_nodes]\n",
    "        nodes_unknown = [node for node in nodes if node not in trainUsersLower and node not in dongle_nodes]\n",
    "    else:\n",
    "        nodes_unknown = [node for node in mention_graph.nodes() if node not in trainUsersLower and node not in dongle_nodes]\n",
    "\n",
    "    # find the cycles with 3 nodes and increase their edge weight\n",
    "    increase_cyclic_edge_weights = False\n",
    "    if increase_cyclic_edge_weights:\n",
    "        increase_coefficient = 2\n",
    "        cycls_3 = [c for c in list(nx.find_cliques(mention_graph)) if len(c) > 2]\n",
    "        print(str(len(cycls_3)) + ' triangles in the graph.')\n",
    "        # cycls_3 = [c for c in nx.cycle_basis(mention_graph) if len(c)==3]\n",
    "        for c_3 in cycls_3:\n",
    "            mention_graph[c_3[0]][c_3[1]]['weight'] *= increase_coefficient\n",
    "            mention_graph[c_3[0]][c_3[2]]['weight'] *= increase_coefficient\n",
    "            mention_graph[c_3[1]][c_3[2]]['weight'] *= increase_coefficient\n",
    "        del cycls_3\n",
    "    \n",
    "    # remove (or decrease the weights of) the edges between training nodes which are very far from each other\n",
    "    remove_inconsistent_edges = False\n",
    "    if remove_inconsistent_edges:\n",
    "        max_acceptable_distance = 161\n",
    "        num_nodes_removed = 0\n",
    "        edges = mention_graph.edges()\n",
    "        edges = [(a, b) for a, b in edges if a in node_location and b in node_location]\n",
    "        for node1, latlon1 in node_location.iteritems():\n",
    "            for node2, latlon2 in node_location.iteritems():\n",
    "                lat1, lon1 = latlon1\n",
    "                lat2, lon2 = latlon2\n",
    "                dd = distance(lat1, lon1, lat2, lon2)\n",
    "                if dd > max_acceptable_distance:\n",
    "                    if ((node1, node2) in edges or (node2, node1) in edges):\n",
    "                        try:\n",
    "                            mention_graph.remove_edge(node1, node2)\n",
    "                            num_nodes_removed += 1\n",
    "                        except:\n",
    "                            pass\n",
    "        print(str(num_nodes_removed) + ' edges removed from the graph') \n",
    "                \n",
    "    use_shortest_paths = False\n",
    "    shortest_paths = {}\n",
    "    if use_shortest_paths:\n",
    "        shortest_paths = nx.all_pairs_shortest_path_length(mention_graph, cutoff=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    logging.info(\"Edge number: \" + str(mention_graph.number_of_edges()))\n",
    "    logging.info(\"Node number: \" + str(mention_graph.number_of_nodes()))\n",
    "    \n",
    "    converged = False\n",
    "    print \"weighted \" + str(weighted)\n",
    "    max_iter = 5\n",
    "    iter_num = 1\n",
    "    print \"iterating with max_iter = \" + str(max_iter)\n",
    "\n",
    "    # if selfish = True, the nodes location would be added to that of its neighbours and then the median is computed. (it didn't improve the results on cmu)\n",
    "    selfish = False\n",
    "    while not converged:\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes)\n",
    "        isolated_users = set()\n",
    "        print \"iter: \" + str(iter_num)\n",
    "        located_nodes_count = len(node_location)\n",
    "        print str(located_nodes_count) + \" nodes have location\"\n",
    "        for node in nodes_unknown:\n",
    "            nbrs = mention_graph[node]\n",
    "            nbrlats = []\n",
    "            nbrlons = []\n",
    "            nbr_edge_weights = []\n",
    "            \n",
    "            if selfish:\n",
    "                if node in node_location:\n",
    "                    self_lat, self_lon = node_location[node]\n",
    "                    nbrlats.append(self_lat)\n",
    "                    nbrlons.append(self_lon)\n",
    "                \n",
    "            for nbr in nbrs:\n",
    "                if nbr in node_location:\n",
    "                    lat, lon = node_location[nbr]\n",
    "                    edge_weight = mention_graph[node][nbr]['weight']\n",
    "                    nbrlats.append(lat)\n",
    "                    nbrlons.append(lon)\n",
    "                    if normalize_edge and weighted:\n",
    "                        edge_weight_normalized = float(edge_weight) / (mention_graph.degree(nbr) * mention_graph.degree(node))\n",
    "                        nbr_edge_weights.append(edge_weight_normalized)\n",
    "                    # elif not weighted:\n",
    "                    #    nbr_edge_weights.append(1)\n",
    "                    else:\n",
    "                        nbr_edge_weights.append(edge_weight)\n",
    "                    \n",
    "            if use_shortest_paths:\n",
    "                if node in shortest_paths:\n",
    "                    community_nbrs = shortest_paths[node]\n",
    "                    for nbr, path_length in community_nbrs.iteritems():\n",
    "                        if path_length > 1:\n",
    "                            if nbr in node_location:\n",
    "                                lat, lon = node_location[nbr]\n",
    "                                nbrlats.append(lat)\n",
    "                                nbrlons.append(lon)\n",
    "            if len(nbrlons) > 0:\n",
    "                nbr_median_lat, nbr_median_lon = weighted_median(nbrlats, nbr_edge_weights), weighted_median(nbrlons, nbr_edge_weights)\n",
    "                node_location[node] = (nbr_median_lat, nbr_median_lon)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            converged = True\n",
    "        \n",
    "        if len(node_location) == located_nodes_count:\n",
    "            print \"converged. No new nodes added in this iteration.\"\n",
    "            # converged = True\n",
    "        distances = []\n",
    "        isolated = 0\n",
    "        for user, loc in evalUsersLower.iteritems():\n",
    "            lat, lon = loc\n",
    "            if user not in node_location:\n",
    "                isolated += 1\n",
    "                isolated_users.add(user)\n",
    "            if backoff and user in isolated_users:\n",
    "                predicted_lat, predicted_lon = text_preds[user]\n",
    "            else:\n",
    "                predicted_lat, predicted_lon = node_location.get(user, (medianLat, medianLon))\n",
    "            # Here get output to LPpredictions\n",
    "            LPpredictions[user]=[predicted_lat, predicted_lon]\n",
    "            dist = distance(lat, lon, predicted_lat, predicted_lon)\n",
    "            distances.append(dist)\n",
    "        current_median = np.median(distances)\n",
    "        current_mean = np.mean(distances)\n",
    "        current_acc = 100 * len([d for d in distances if d < 161]) / float(len(distances))\n",
    "\n",
    "        print \"mean: \" + str(int(current_mean))\n",
    "        print \"median:\" + str(int(current_median))\n",
    "        print \"Acc@161:\" + str(current_acc)\n",
    "        print \"isolated test users are \" + str(isolated) + \" out of \" + str(len(distances))\n",
    "\n",
    "    return current_mean, current_median, current_acc\n",
    "    \n",
    "\n",
    "def LP_collapsed(weighted=True, prior='none', normalize_edge=False, remove_celebrities=False, dev=False, project_to_main_users=False, node_order='l2h', remove_mentions_with_degree_one=True):\n",
    "    '''\n",
    "    runs label propagation over the collapsed @-mention graph.\n",
    "    In a collapsed @-mention graph, @-mentions which are not a member of target nodes (training,dev or test users) are\n",
    "    removed from the graph. The target nodes which were previously connected through the removed @-mention nodes\n",
    "    will be connected with an edge.\n",
    "    If project_to_main_users is False the graph won't be collapsed and the complete @-mention graph will be used.\n",
    "    '''\n",
    "    U_train = [u for u in sorted(trainUsers)]\n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "    U_eval = []\n",
    "    if dev:\n",
    "        evalText = devText\n",
    "        evalUsers = devUsers\n",
    "        U_eval = U_dev\n",
    "    else:\n",
    "        evalText = testText\n",
    "        evalUsers = testUsers\n",
    "        U_eval = U_test\n",
    "\n",
    "    U_all = U_train + U_eval\n",
    "    assert len(U_all) == len(U_train) + len(U_eval), \"duplicate user problem\"\n",
    "    idx = range(len(U_all))\n",
    "    node_id = dict(zip(U_all, idx))\n",
    "    \n",
    "    mention_graph = nx.Graph()\n",
    "    \n",
    "    print('weighted=%s and prior=%s' % (weighted, prior))\n",
    "\n",
    "    print \"building the direct graph\"\n",
    "    token_pattern1 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern1 = re.compile(token_pattern1)\n",
    "    token_pattern2 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))#([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern2 = re.compile(token_pattern2)\n",
    "    l = len(trainText)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    # add train and test users to the graph\n",
    "    mention_graph.add_nodes_from(node_id.values())\n",
    "    for user, text in trainText.iteritems():\n",
    "        user_id = node_id[user]    \n",
    "        if i % tenpercent == 0:\n",
    "            print str(10 * i / tenpercent) + \"%\"\n",
    "        i += 1  \n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id] \n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)   \n",
    "       \n",
    "    print \"adding the eval graph\"\n",
    "    for user, text in evalText.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id]\n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)  \n",
    "        \n",
    "    \n",
    "    trainuserid_location = {}\n",
    "    evaluserid_location = {}\n",
    "    trainLats = []\n",
    "    trainLons = []\n",
    "    node_location = {}\n",
    "    dongle_nodes = []\n",
    "    text_preds = {}\n",
    "    for user, loc in trainUsers.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        trainLats.append(lat)\n",
    "        trainLons.append(lon)\n",
    "        trainuserid_location[user_id] = (lat, lon)\n",
    "        node_location[user_id] = (lat, lon)\n",
    "        \n",
    "    for user, loc in evalUsers.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        evaluserid_location[user_id] = (lat, lon)\n",
    "    \n",
    "    print \"the number of train nodes is \" + str(len(trainUsers))\n",
    "    print \"the number of test nodes is \" + str(len(evalUsers))\n",
    "    medianLat = np.median(trainLats)\n",
    "    medianLon = np.median(trainLons)\n",
    "\n",
    "    celebrity_threshold = celeb_threshold\n",
    "    celebrities = []\n",
    "    if remove_celebrities:\n",
    "        nodes = mention_graph.nodes_iter()\n",
    "        for node in nodes:\n",
    "            nbrs = mention_graph.neighbors(node)\n",
    "            if len(nbrs) > celebrity_threshold:\n",
    "                if node not in evaluserid_location and node not in trainuserid_location:\n",
    "                    celebrities.append(node)\n",
    "        print(\"found %d celebrities with celebrity threshold %d\" % (len(celebrities), celebrity_threshold))\n",
    "        for celebrity in celebrities:\n",
    "                mention_graph.remove_node(celebrity)\n",
    "    \n",
    "    dongle = True\n",
    "    backoff = False\n",
    "    text_direct = False\n",
    "    if prior != 'none':\n",
    "        if prior == 'backoff':\n",
    "            backoff = True\n",
    "        elif prior == 'prior':\n",
    "            text_direct = True\n",
    "        elif prior == 'dongle':\n",
    "            dongle = True\n",
    "        prior_file_path = \"C:/Users/Dennis/Desktop/geolocation/dump/dump.pkl\"\n",
    "        print \"reading prior text-based locations from \" + prior_file_path\n",
    "        if os.path.exists(prior_file_path):\n",
    "            with open(prior_file_path, 'rb') as inf:\n",
    "                preds, devPreds, U_test, U_dev, testProbs, devProbs = pickle.load(inf)\n",
    "                if dev:\n",
    "                    preds = devPreds\n",
    "                    U_test = U_dev\n",
    "                    testProbs = devProbs\n",
    "                test_confidences = testProbs[np.arange(0, preds.shape[0]), preds]\n",
    "                loss(preds=preds, U_test=U_test)\n",
    "                if dongle:\n",
    "                    mention_graph.add_nodes_from([u + '.dongle' for u in U_test])\n",
    "                user_index = 0   \n",
    "                for user, pred in zip(U_test, preds):\n",
    "                    user_id = node_id[user]\n",
    "                    lat = classLatMedian[str(pred)]\n",
    "                    lon = classLonMedian[str(pred)]\n",
    "                    if backoff:\n",
    "                        text_preds[user_id] = (lat, lon)\n",
    "                    if dongle:\n",
    "                        dongle_node = user_id + '.dongle'\n",
    "                        w = test_confidences[user_index]\n",
    "                        mention_graph.add_edge(dongle_node, user_id, weight=w)\n",
    "                        node_location[dongle_node] = (lat, lon)\n",
    "                        dongle_nodes.append(dongle_node)\n",
    "                    elif text_direct:\n",
    "                        node_location[user_id] = (lat, lon)\n",
    "                    user_index += 1\n",
    "        else:\n",
    "            print \"prior file not found.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if remove_mentions_with_degree_one:\n",
    "        mention_nodes = set(mention_graph.nodes()) - set(node_id.values())\n",
    "        mention_degree = mention_graph.degree(nbunch=mention_nodes, weight=None)\n",
    "        one_degree_non_target = {node for node, degree in mention_degree.iteritems() if degree < 2}\n",
    "        logging.info('found ' + str(len(one_degree_non_target)) + ' mentions with degree 1 in the graph.')\n",
    "        for node in one_degree_non_target:\n",
    "            mention_graph.remove_node(node)\n",
    "            \n",
    "    if project_to_main_users:\n",
    "        logging.info('projecting the graph into the target user.')\n",
    "        main_users = node_id.values()\n",
    "        # mention_graph = bipartite.overlap_weighted_projected_graph(mention_graph, main_users, jaccard=False)\n",
    "        mention_graph = collaboration_weighted_projected_graph(mention_graph, main_users, weight_str=None, degree_power=1, caller='lp')\n",
    "        # mention_graph = collaboration_weighted_projected_graph(mention_graph, main_users, weight_str='weight')\n",
    "        # mention_graph = bipartite.projected_graph(mention_graph, main_users)\n",
    "    logging.info(\"Edge number: \" + str(mention_graph.number_of_edges()))\n",
    "    logging.info(\"Node number: \" + str(mention_graph.number_of_nodes()))\n",
    "    # results[str(project_to_main_users)] = mention_graph.degree().values()\n",
    "    # return\n",
    "    remove_betweeners = False\n",
    "    if remove_betweeners:\n",
    "        print(\"computing betweenness centrality of all nodes, takes a long time, sorry!\")\n",
    "        scores = nx.betweenness_centrality(mention_graph, weight='weight')\n",
    "        i = 0\n",
    "        percent_5 = len(scores) / 20 \n",
    "        for w in sorted(scores, key=scores.get, reverse=True):\n",
    "            i += 1\n",
    "            if i < percent_5:\n",
    "                mention_graph.remove_node(w)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # check the effect of geolocation order on performance. The intuition is that\n",
    "    # if the high confident nodes are geolocated first it may be better because noisy\n",
    "    # predictions won't propagate from the first iterations.\n",
    "    if node_order:\n",
    "        node_degree = mention_graph.degree()\n",
    "        if node_order == 'h2l':\n",
    "            reverse_order = True\n",
    "        else:\n",
    "            reverse_order = False\n",
    "        nodes = sorted(node_degree, key=node_degree.get, reverse=reverse_order)\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes)\n",
    "        # nodes_unknown = [node for node in mention_graph.nodes() if node not in trainUsersLower and node not in dongle_nodes]\n",
    "        nodes_unknown = [node for node in nodes if node not in trainuserid_location and node not in dongle_nodes]\n",
    "    else:\n",
    "        nodes_unknown = [node for node in mention_graph.nodes() if node not in trainuserid_location and node not in dongle_nodes]\n",
    "    # find the cycles with 3 nodes and increase their edge weight\n",
    "    increase_cyclic_edge_weights = False\n",
    "    if increase_cyclic_edge_weights:\n",
    "        increase_coefficient = 2\n",
    "        cycls_3 = [c for c in list(nx.find_cliques(mention_graph)) if len(c) > 2]\n",
    "        print(str(len(cycls_3)) + ' triangles in the graph.')\n",
    "        # cycls_3 = [c for c in nx.cycle_basis(mention_graph) if len(c)==3]\n",
    "        for c_3 in cycls_3:\n",
    "            mention_graph[c_3[0]][c_3[1]]['weight'] *= increase_coefficient\n",
    "            mention_graph[c_3[0]][c_3[2]]['weight'] *= increase_coefficient\n",
    "            mention_graph[c_3[1]][c_3[2]]['weight'] *= increase_coefficient\n",
    "        del cycls_3\n",
    "    \n",
    "    # remove (or decrease the weights of) the edges between training nodes which are very far from each other\n",
    "    remove_inconsistent_edges = False\n",
    "    if remove_inconsistent_edges:\n",
    "        max_acceptable_distance = 161\n",
    "        num_nodes_removed = 0\n",
    "        edges = mention_graph.edges()\n",
    "        edges = [(a, b) for a, b in edges if a in node_location and b in node_location]\n",
    "        for node1, latlon1 in node_location.iteritems():\n",
    "            for node2, latlon2 in node_location.iteritems():\n",
    "                lat1, lon1 = latlon1\n",
    "                lat2, lon2 = latlon2\n",
    "                dd = distance(lat1, lon1, lat2, lon2)\n",
    "                if dd > max_acceptable_distance:\n",
    "                    if ((node1, node2) in edges or (node2, node1) in edges):\n",
    "                        try:\n",
    "                            mention_graph.remove_edge(node1, node2)\n",
    "                            num_nodes_removed += 1\n",
    "                        except:\n",
    "                            pass\n",
    "        print(str(num_nodes_removed) + ' edges removed from the graph') \n",
    "                \n",
    "    use_shortest_paths = False\n",
    "    shortest_paths = {}\n",
    "    if use_shortest_paths:\n",
    "        shortest_paths = nx.all_pairs_shortest_path_length(mention_graph, cutoff=3)\n",
    "    \n",
    "    \n",
    "    converged = False\n",
    "    logging.info(\"weighted \" + str(weighted))\n",
    "    max_iter = 5\n",
    "    iter_num = 1\n",
    "    logging.info(\"iterating with max_iter = \" + str(max_iter))\n",
    "    # if selfish = True, the nodes location would be added to that of its neighbours and then the median is computed. (it didn't improve the results on cmu)\n",
    "    selfish = False\n",
    "    while not converged:\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes_unknown)\n",
    "        isolated_users = set()\n",
    "        print \"iter: \" + str(iter_num)\n",
    "        located_nodes_count = len(node_location)\n",
    "        logging.info(str(located_nodes_count) + \" nodes have location\")\n",
    "        for node in nodes_unknown:\n",
    "            nbrs = mention_graph[node]\n",
    "            nbrlats = []\n",
    "            nbrlons = []\n",
    "            nbr_edge_weights = []\n",
    "            \n",
    "            if selfish:\n",
    "                if node in node_location:\n",
    "                    self_lat, self_lon = node_location[node]\n",
    "                    nbrlats.append(self_lat)\n",
    "                    nbrlons.append(self_lon)\n",
    "                \n",
    "            for nbr in nbrs:\n",
    "                if nbr in node_location:\n",
    "                    lat, lon = node_location[nbr]\n",
    "                    edge_weight = mention_graph[node][nbr]['weight']\n",
    "                    nbrlats.append(lat)\n",
    "                    nbrlons.append(lon)\n",
    "                    if normalize_edge and weighted:\n",
    "                        edge_weight_normalized = float(edge_weight) / (mention_graph.degree(nbr) * mention_graph.degree(node))\n",
    "                        nbr_edge_weights.append(edge_weight_normalized)\n",
    "                    # elif not weighted:\n",
    "                    #    nbr_edge_weights.append(1)\n",
    "                    else:\n",
    "                        nbr_edge_weights.append(edge_weight)\n",
    "                    \n",
    "            if use_shortest_paths:\n",
    "                if node in shortest_paths:\n",
    "                    community_nbrs = shortest_paths[node]\n",
    "                    for nbr, path_length in community_nbrs.iteritems():\n",
    "                        if path_length > 1:\n",
    "                            if nbr in node_location:\n",
    "                                lat, lon = node_location[nbr]\n",
    "                                nbrlats.append(lat)\n",
    "                                nbrlons.append(lon)\n",
    "            if len(nbrlons) > 0:\n",
    "                nbr_median_lat, nbr_median_lon = weighted_median(nbrlats, nbr_edge_weights), weighted_median(nbrlons, nbr_edge_weights)\n",
    "                node_location[node] = (nbr_median_lat, nbr_median_lon)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            converged = True\n",
    "        \n",
    "        if len(node_location) == located_nodes_count:\n",
    "            logging.info(\"converged. No new nodes added in this iteration.\")\n",
    "            # converged = True\n",
    "        distances = []\n",
    "        isolated = 0\n",
    "        for evaluser, loc in evalUsers.iteritems():\n",
    "            lat, lon = locationStr2Float(loc)\n",
    "            evaluserid = node_id[evaluser]\n",
    "            if evaluserid not in node_location:\n",
    "                isolated += 1\n",
    "                isolated_users.add(evaluserid)\n",
    "            if backoff and evaluserid in isolated_users:\n",
    "                predicted_lat, predicted_lon = text_preds[evaluserid]\n",
    "            else:\n",
    "                predicted_lat, predicted_lon = node_location.get(evaluserid, (medianLat, medianLon))\n",
    "            LPpredictions[user]=[predicted_lat, predicted_lon]\n",
    "            dist = distance(lat, lon, predicted_lat, predicted_lon)\n",
    "            distances.append(dist)\n",
    "        current_median = np.median(distances)\n",
    "        current_mean = np.mean(distances)\n",
    "        current_acc = 100 * len([d for d in distances if d < 161]) / float(len(distances))\n",
    "        logging.info(\"mean: \" + str(int(current_mean)))\n",
    "        logging.info(\"median:\" + str(int(current_median)))\n",
    "        logging.info(\"Acc@161:\" + str(current_acc))\n",
    "        logging.info(\"isolated test users are \" + str(isolated) + \" out of \" + str(len(distances)))\n",
    "        \n",
    "        compute_degree_error = False\n",
    "        if compute_degree_error:\n",
    "            degrees = []\n",
    "            user_degree = mention_graph.degree([node_id[u] for u in evalUsers.keys()])\n",
    "            for u in evalUsers:\n",
    "                evalUserId = node_id[u]\n",
    "                degree = user_degree[evalUserId]\n",
    "                degrees.append(degree)\n",
    "    \n",
    "    return current_mean, current_median, current_acc\n",
    "\n",
    "def LP_classification(weighted=True, prior='none', normalize_edge=False, remove_celebrities=False, dev=False, project_to_main_users=False, node_order='l2h', remove_mentions_with_degree_one=True):\n",
    "    '''\n",
    "    This function implements iterative label propagation as in Modified Adsorption without the regulariser term.\n",
    "    This is not parallel and is slower than running Junto.\n",
    "    The labels are the result of running k-d tree over the training coordinates which discretises\n",
    "    the real-valued coordinates into several regions with different area but the same number of users.\n",
    "    The label distribution of training users are kept unchanged. The label distribution of test/dev users\n",
    "    are updated to the mean of their neighbours.\n",
    "    If project_to_main_users is True the network will be collapsed (keeping just training and test/dev users) and\n",
    "    if False, the complete @-mention graph will be used.\n",
    "    Note: The results reported in the paper are not based on this function. They are based on label propagation\n",
    "    using Modified Adsorption using Junto implementation (https://github.com/parthatalukdar/junto).\n",
    "    '''\n",
    "    U_train = [u for u in sorted(trainUsers)]\n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "    num_classes = len(categories)\n",
    "    logging.info('running classification based label propagation') \n",
    "    U_eval = []\n",
    "    if dev:\n",
    "        evalText = devText\n",
    "        evalUsers = devUsers\n",
    "        U_eval = U_dev\n",
    "        eval_classes = devClasses\n",
    "    else:\n",
    "        evalText = testText\n",
    "        evalUsers = testUsers\n",
    "        U_eval = U_test\n",
    "        eval_classes = testClasses\n",
    "    save_gr = False\n",
    "    U_all = U_train + U_eval\n",
    "    assert len(U_all) == len(U_train) + len(U_eval), \"duplicate user problem\"\n",
    "    idx = range(len(U_all))\n",
    "    node_id = dict(zip(U_all, idx))\n",
    "\n",
    "\n",
    "    \n",
    "    mention_graph = nx.Graph()\n",
    "    graph_file_address = path.join(GEOTEXT_HOME, 'direct_graph.graphml')\n",
    "    \n",
    "    print('weighted=%s and prior=%s' % (weighted, prior))\n",
    "\n",
    "    print \"building the direct graph\"\n",
    "    token_pattern1 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern1 = re.compile(token_pattern1)\n",
    "    token_pattern2 = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))#([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "    token_pattern2 = re.compile(token_pattern2)\n",
    "    l = len(trainText)\n",
    "    tenpercent = l / 10\n",
    "    i = 1\n",
    "    # add train and test users to the graph\n",
    "    mention_graph.add_nodes_from(node_id.values())\n",
    "    for user, text in trainText.iteritems():\n",
    "        user_id = node_id[user]    \n",
    "        if i % tenpercent == 0:\n",
    "            print str(10 * i / tenpercent) + \"%\"\n",
    "        i += 1  \n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id] \n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)   \n",
    "       \n",
    "    print \"adding the eval graph\"\n",
    "    for user, text in evalText.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        mentions = [node_id.get(u.lower(), u.lower()) for u in token_pattern1.findall(text)]\n",
    "        mentions = [m for m in mentions if m != user_id]\n",
    "        mentionDic = Counter(mentions)\n",
    "        mention_graph.add_nodes_from(mentionDic.keys())\n",
    "        for mention, freq in mentionDic.iteritems():\n",
    "            if not weighted:\n",
    "                freq = 1\n",
    "            if mention_graph.has_edge(user_id, mention):\n",
    "                mention_graph[user_id][mention]['weight'] += freq\n",
    "                # mention_graph[mention][user]['weight'] += freq/2.0\n",
    "            else:\n",
    "                mention_graph.add_edge(user_id, mention, weight=freq)\n",
    "                # mention_graph.add_edge(mention, user, weight=freq/2.0)  \n",
    "        \n",
    "    \n",
    "    trainuserid_location = {}\n",
    "    evaluserid_location = {}\n",
    "    trainLats = []\n",
    "    trainLons = []\n",
    "    node_location = {}\n",
    "    dongle_nodes = []\n",
    "    text_preds = {}\n",
    "    \n",
    "    for user, loc in trainUsers.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        trainLats.append(lat)\n",
    "        trainLons.append(lon)\n",
    "        trainuserid_location[user_id] = (lat, lon)\n",
    "        \n",
    "    for user, loc in evalUsers.iteritems():\n",
    "        user_id = node_id[user]\n",
    "        lat, lon = locationStr2Float(loc)\n",
    "        evaluserid_location[user_id] = (lat, lon)\n",
    "    \n",
    "    print \"the number of train nodes is \" + str(len(trainUsers))\n",
    "    print \"the number of test nodes is \" + str(len(evalUsers))\n",
    "    median_lat = np.median(trainLats)\n",
    "    median_lon = np.median(trainLons)\n",
    "    median_classIndx, dist = assignClass(median_lat, median_lon)\n",
    "    celebrity_threshold = celeb_threshold\n",
    "    celebrities = []\n",
    "    if remove_celebrities:\n",
    "        nodes = mention_graph.nodes()\n",
    "        for node in nodes:\n",
    "            nbrs = mention_graph.neighbors(node)\n",
    "            if len(nbrs) > celebrity_threshold:\n",
    "                if node not in node_id.values():\n",
    "                    celebrities.append(node)\n",
    "        print(\"found %d celebrities with celebrity threshold %d\" % (len(celebrities), celebrity_threshold))\n",
    "        for celebrity in celebrities:\n",
    "                mention_graph.remove_node(celebrity)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if remove_mentions_with_degree_one:\n",
    "        mention_nodes = set(mention_graph.nodes()) - set(node_id.values())\n",
    "        mention_degree = mention_graph.degree(nbunch=mention_nodes, weight=None)\n",
    "        one_degree_non_target = {node for node, degree in mention_degree.iteritems() if degree < 2}\n",
    "        logging.info('found ' + str(len(one_degree_non_target)) + ' mentions with degree 1 in the graph.')\n",
    "        for node in one_degree_non_target:\n",
    "            mention_graph.remove_node(node)\n",
    "            \n",
    "    if project_to_main_users:\n",
    "        logging.info('projecting the graph into the target user.')\n",
    "        main_users = node_id.values()\n",
    "        # mention_graph = bipartite.overlap_weighted_projected_graph(mention_graph, main_users, jaccard=False)\n",
    "        mention_graph = collaboration_weighted_projected_graph(mention_graph, main_users, weight_str=None, degree_power=1, caller='lp')\n",
    "        # mention_graph = collaboration_weighted_projected_graph(mention_graph, main_users, weight_str='weight')\n",
    "        # mention_graph = bipartite.projected_graph(mention_graph, main_users)\n",
    "    logging.info(\"Edge number: \" + str(mention_graph.number_of_edges()))\n",
    "    logging.info(\"Node number: \" + str(mention_graph.number_of_nodes()))\n",
    "\n",
    "\n",
    "    node_labeldist = {}\n",
    "    logging.info('initialising user label distributions...')\n",
    "    for node, id in node_id.iteritems():\n",
    "        if id < len(U_train):\n",
    "            label = trainClasses[node]\n",
    "            dist = lil_matrix((1, len(categories)))\n",
    "            dist[0, label] = 1\n",
    "            node_labeldist[id] = dist\n",
    "        \n",
    "    \n",
    "    # check the effect of geolocation order on performance. The intuition is that\n",
    "    # if the high confident nodes are geolocated first it may be better because noisy\n",
    "    # predictions won't propagate from the first iterations.\n",
    "    if node_order:\n",
    "        node_degree = mention_graph.degree()\n",
    "        if node_order == 'h2l':\n",
    "            reverse_order = True\n",
    "        else:\n",
    "            reverse_order = False\n",
    "        nodes = sorted(node_degree, key=node_degree.get, reverse=reverse_order)\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes)\n",
    "        # nodes_unknown = [node for node in mention_graph.nodes() if node not in trainUsersLower and node not in dongle_nodes]\n",
    "        nodes_unknown = [node for node in nodes if node not in trainuserid_location and node not in dongle_nodes]\n",
    "    else:\n",
    "        nodes_unknown = [node for node in mention_graph.nodes() if node not in trainuserid_location and node not in dongle_nodes]\n",
    "\n",
    "    converged = False\n",
    "    logging.info(\"weighted \" + str(weighted))\n",
    "    max_iter = 2\n",
    "    iter_num = 1\n",
    "    logging.info(\"iterating with max_iter = \" + str(max_iter))\n",
    "    while not converged:\n",
    "        if node_order == 'random':\n",
    "            random.shuffle(nodes_unknown)\n",
    "        isolated_users = set()\n",
    "        print \"iter: \" + str(iter_num)\n",
    "        located_nodes_count = len(node_labeldist)\n",
    "        logging.info(str(located_nodes_count) + \" nodes have location\")\n",
    "        for node in nodes_unknown:\n",
    "            should_be_updated = False\n",
    "            neighbors_labeldist = lil_matrix((1, len(categories)))\n",
    "            nbrs = mention_graph[node]    \n",
    "            for nbr, edge_data in nbrs.iteritems():\n",
    "                if nbr in node_labeldist:\n",
    "                    should_be_updated = True\n",
    "                    nbrlabeldist = node_labeldist[nbr]\n",
    "                    edge_weight = edge_data['weight']\n",
    "                    neighbors_labeldist = neighbors_labeldist + edge_weight * nbrlabeldist\n",
    "            if should_be_updated:\n",
    "                old_labeldist = node_labeldist.get(node, csr_matrix((1, len(categories))))\n",
    "                new_labeldist = old_labeldist + neighbors_labeldist\n",
    "                # inplace normalization\n",
    "                normalize(new_labeldist, norm='l1', copy=False)\n",
    "                node_labeldist[node] = new_labeldist\n",
    "        eval_predictions = []\n",
    "        eval_confidences = []\n",
    "        for u in U_eval:\n",
    "            u_id = node_id[u]\n",
    "            prediction = -1\n",
    "            confidence = 0\n",
    "            if u_id in node_labeldist:\n",
    "                labeldist = node_labeldist[u_id].toarray()\n",
    "                prediction = np.argmax(labeldist)\n",
    "                confidence = np.max(labeldist) \n",
    "            else:\n",
    "                isolated_users.add(u_id)\n",
    "                prediction = int(median_classIndx)\n",
    "            eval_predictions.append(prediction)\n",
    "        loss(preds=eval_predictions, U_test=U_eval, save_results_bucket_size=True, results_key=('classification', project_to_main_users, node_order)) \n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            converged = True\n",
    "        \n",
    "        if len(node_labeldist) == located_nodes_count:\n",
    "            logging.info(\"converged. No new nodes added in this iteration.\")\n",
    "            # converged = True\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def collaboration_weighted_projected_graph(B, nodes, weight_str=None, degree_power=1, caller='lp'):\n",
    "    r\"\"\"Newman's weighted projection of B onto one of its node sets.\n",
    "\n",
    "    The collaboration weighted projection is the projection of the\n",
    "    bipartite network B onto the specified nodes with weights assigned\n",
    "    using Newman's collaboration model [1]_:\n",
    "\n",
    "    .. math::\n",
    "        \n",
    "        w_{v,u} = \\sum_k \\frac{\\delta_{v}^{w} \\delta_{w}^{k}}{k_w - 1}\n",
    "\n",
    "    where `v` and `u` are nodes from the same bipartite node set,\n",
    "    and `w` is a node of the opposite node set. \n",
    "    The value `k_w` is the degree of node `w` in the bipartite\n",
    "    network and `\\delta_{v}^{w}` is 1 if node `v` is\n",
    "    linked to node `w` in the original bipartite graph or 0 otherwise.\n",
    " \n",
    "    The nodes retain their attributes and are connected in the resulting\n",
    "    graph if have an edge to a common node in the original bipartite\n",
    "    graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : NetworkX graph \n",
    "      The input graph should be bipartite. \n",
    "\n",
    "    nodes : list or iterable\n",
    "      Nodes to project onto (the \"bottom\" nodes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Graph : NetworkX graph \n",
    "       A graph that is the projection onto the given nodes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from networkx.algorithms import bipartite\n",
    "    >>> B = nx.path_graph(5)\n",
    "    >>> B.add_edge(1,5)\n",
    "    >>> G = bipartite.collaboration_weighted_projected_graph(B, [0, 2, 4, 5])\n",
    "    >>> print(G.nodes())\n",
    "    [0, 2, 4, 5]\n",
    "    >>> for edge in G.edges(data=True): print(edge)\n",
    "    ... \n",
    "    (0, 2, {'weight': 0.5})\n",
    "    (0, 5, {'weight': 0.5})\n",
    "    (2, 4, {'weight': 1.0})\n",
    "    (2, 5, {'weight': 0.5})\n",
    "    \n",
    "    Notes\n",
    "    ------\n",
    "    No attempt is made to verify that the input graph B is bipartite.\n",
    "    The graph and node properties are (shallow) copied to the projected graph.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    is_bipartite, \n",
    "    is_bipartite_node_set, \n",
    "    sets, \n",
    "    weighted_projected_graph,\n",
    "    overlap_weighted_projected_graph,\n",
    "    generic_weighted_projected_graph,\n",
    "    projected_graph \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Scientific collaboration networks: II. \n",
    "        Shortest paths, weighted networks, and centrality, \n",
    "        M. E. J. Newman, Phys. Rev. E 64, 016132 (2001).\n",
    "    \"\"\"\n",
    "    if B.is_multigraph():\n",
    "        raise nx.NetworkXError(\"not defined for multigraphs\")\n",
    "    if B.is_directed():\n",
    "        pred = B.pred\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        pred = B.adj\n",
    "        G = nx.Graph()\n",
    "    G.graph.update(B.graph)\n",
    "    G.add_nodes_from((n, B.node[n]) for n in nodes)\n",
    "    direct_edge_counter = 0\n",
    "    for v1, v2 in B.edges_iter(data=False):\n",
    "        if type(v1) == int and type(v2) == int:\n",
    "            w = (1.0 / len(B[v1]) + 1.0 / len(B[v2]))\n",
    "            G.add_edge(v1, v2, weight=w) \n",
    "            direct_edge_counter += 1\n",
    "    logging.info('direct edge count: ' + str(direct_edge_counter))\n",
    "    i = 0\n",
    "    tenpercent = len(nodes) / 10\n",
    "    for n1 in nodes:\n",
    "        if i % tenpercent == 0:\n",
    "            logging.info(str(10 * i / tenpercent) + \"%\")\n",
    "        i += 1  \n",
    "        unbrs = set(B[n1])\n",
    "        nbrs2 = set((n for nbr in unbrs for n in B[nbr])) - set([n1])\n",
    "        nbrs2 = [n for n in nbrs2 if type(n) == int]\n",
    "            # pass\n",
    "        for n2 in nbrs2:\n",
    "            weight = 0\n",
    "            if G.has_edge(n1, n2):\n",
    "                weight += G[n1][n2]['weight'] \n",
    "            vnbrs = set(pred[n2])\n",
    "            common = unbrs & vnbrs\n",
    "            del vnbrs\n",
    "\n",
    "            if weight_str is not None:\n",
    "                weight += sum([1.0 * B[n1][n][weight_str] * B[n2][n][weight_str] / ((len(B[n]) - 1) ** degree_power) for n in common if len(B[n]) > 1])\n",
    "            else:\n",
    "                    weight += sum([1.0 / ((len(B[n]) - 1) ** degree_power) for n in common if len(B[n]) > 1])\n",
    "            if weight != 0:\n",
    "                G.add_edge(n1, n2, weight=weight)\n",
    "    \n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def junto_postprocessing(multiple=False, dev=False, method='median', celeb_threshold=5, weighted=False, text_prior='none', postfix=''):\n",
    "    EVALUATE_REAL_VALUED = False\n",
    "\n",
    "    lats = classLatMedian.values()\n",
    "    lons = classLonMedian.values()\n",
    "    median_lat = np.median(lats)\n",
    "    median_lon = np.median(lons)\n",
    "    classIndx, dist = assignClass(median_lat, median_lon)\n",
    "    trainUsersLower = [u.lower() for u in sorted(trainUsers)]\n",
    "    \n",
    "    \n",
    "    U_test = [u for u in sorted(testUsers)]\n",
    "    U_dev = [u for u in sorted(devUsers)]\n",
    "    if dev:\n",
    "        U_eval = U_dev\n",
    "        devStr = '.dev'\n",
    "    else:\n",
    "        U_eval = U_test\n",
    "        devStr = ''\n",
    "    result_dump_file = \"C:/Users/Dennis/Desktop/geolocation/world/predsTest.pkl\"\n",
    "    if text_prior != 'none':\n",
    "        logging.info(\"reading (preds, devPreds, U_test, U_dev, testProbs, devProbs) from \" + result_dump_file)\n",
    "        with open(result_dump_file, 'rb') as inf:\n",
    "            preds_text, devPreds_text, U_test_text, U_dev_text, testProbs_text, devProbs_text = pickle.load(inf)\n",
    "            logging.info(\"text test results:\")\n",
    "            loss(preds_text, U_test_text)\n",
    "            logging.info(\"text dev results:\")\n",
    "            #loss(devPreds_text, U_dev_text)\n",
    "            \n",
    "            if dev:\n",
    "                text_preds = devPreds_text\n",
    "                text_probs = devProbs_text\n",
    "                assert U_dev == U_dev_text, \"text users are not equal to loaded dev users\"\n",
    "                \n",
    "            else:\n",
    "                text_preds = preds_text\n",
    "                text_probs = testProbs_text\n",
    "                assert U_test == U_test_text, \"text users are not equal to loaded test users\"\n",
    "    \n",
    "    # split a training set and a test set\n",
    "    Y_test = np.asarray([testClasses[u] for u in U_test])\n",
    "    Y_dev = np.asarray([devClasses[u] for u in U_dev])\n",
    "    \n",
    "    textStr = '.' + text_prior\n",
    "    if text_prior == 'none':\n",
    "        textStr = ''\n",
    "    weightedStr = '.weighted'\n",
    "    if not weighted:\n",
    "        weightedStr = ''\n",
    "    \n",
    "\n",
    "        \n",
    "    if not multiple:\n",
    "        \n",
    "        files = [path.join(GEOTEXT_HOME, 'label_prop_output_' + DATASETS[DATASET_NUMBER - 1] + '_' + method + '_' + str(BUCKET_SIZE) + '_' + str(celeb_threshold) + devStr + textStr + weightedStr + postfix)]\n",
    "    else:\n",
    "        junto_output_dir = GEOTEXT_HOME+ DATASETS[DATASET_NUMBER - 1]\n",
    "        files = glob.glob(junto_output_dir + '/label_prop_output*')\n",
    "        files = sorted(files)\n",
    "    # feature_extractor2(min_df=50)\n",
    "\n",
    "    for junto_output_file in files:   \n",
    "        id_name_file = path.join(GEOTEXT_HOME, 'id_user' + '_' + method + '_' + str(BUCKET_SIZE) + devStr + textStr + weightedStr) \n",
    "        logging.info(\"output file: \" + junto_output_file)\n",
    "        logging.info(\"id_name file: \" + id_name_file)\n",
    "        name_id = {}\n",
    "        id_pred = {}\n",
    "        name_pred = {}\n",
    "        id_name = {}\n",
    "        with codecs.open(id_name_file, 'r', 'utf-8') as inf:\n",
    "            for line in inf:\n",
    "                fields = line.split()\n",
    "                name_id[fields[1]] = fields[0]\n",
    "                id_name[fields[0]] = fields[1]\n",
    "        \n",
    "\n",
    "        dummy_count = 0\n",
    "        with codecs.open(junto_output_file, 'r', 'utf-8') as inf:  \n",
    "            print junto_output_file               \n",
    "            # real valued results were not good\n",
    "            if EVALUATE_REAL_VALUED:\n",
    "                distances = []\n",
    "                for line in inf:\n",
    "                    fields = line.split()\n",
    "                    if len(fields) == 11:\n",
    "                        uid = fields[0]\n",
    "                        if '.T' in uid:\n",
    "                            continue\n",
    "                        lat = float(fields[4])\n",
    "                        lon = float(fields[8])\n",
    "                        u = U_test[int(uid) - len(trainUsers)]\n",
    "                        \n",
    "                        lat2, lon2 = locationStr2Float(userLocation[u])\n",
    "                        distances.append(distance(lat, lon, lat2, lon2))\n",
    "                print \"results\"\n",
    "                print str(np.mean(distances))\n",
    "                print str(np.median(distances))\n",
    "            else:\n",
    "                nopred = []\n",
    "                for line in inf:\n",
    "                    fields = line.split('\\t')\n",
    "                    uid = fields[0]\n",
    "                    if '.T' in uid:\n",
    "                        continue\n",
    "                    u = id_name[uid]\n",
    "                    test_user_inconsistency = 0\n",
    "                    second_option_selected = 0\n",
    "                    if u in U_eval:\n",
    "                        user_index = U_eval.index(u)\n",
    "                        try:\n",
    "                            label_scores = fields[-3]\n",
    "                            label = label_scores.split()[0]\n",
    "                            labelProb = float(label_scores.split()[1])\n",
    "                            if label == '__DUMMY__':\n",
    "                                # pass\n",
    "                                logging.info('choosing second ranked label as the first one is a dummy!')\n",
    "                                label = label_scores.split()[2]\n",
    "                                labelProb = float(label_scores.split()[2])\n",
    "                                second_option_selected += 1\n",
    "                        except:\n",
    "                            print fields\n",
    "                        \n",
    "                        # Tracer()()\n",
    "                        if label == '__DUMMY__':\n",
    "                            dummy_count += 1\n",
    "                            if text_prior == 'backoff':\n",
    "                                text_predition = str(text_preds[user_index])\n",
    "                                label = text_predition\n",
    "                            else:\n",
    "                                # label = str(len(categories) / 2)\n",
    "                                label = str(classIndx)\n",
    "                                \n",
    "                        id_pred[fields[0]] = label\n",
    "                    else:\n",
    "                        if u not in trainUsersLower:\n",
    "                            nopred.append((uid, u))\n",
    "                if len(nopred) > 0:\n",
    "                    print \"no predition for these nodes:\" + str(nopred)\n",
    "                    logging.info(\"no prediction for the above nodes.\")\n",
    "                    sys.exit()\n",
    "                logging.info(\"users with second predicted label: \" + str(second_option_selected))\n",
    "                \n",
    "\n",
    "                preds = []\n",
    "                user_not_in_network = 0\n",
    "                doubles_found = 0\n",
    "                for u in U_eval:\n",
    "                    if u.lower() not in name_id:\n",
    "                        ud = u + '_double00'\n",
    "                        doubles_found += 1\n",
    "                        uid = name_id[ud.lower()]\n",
    "                    else:\n",
    "                        uid = name_id[u.lower()]\n",
    "                    if uid in id_pred:\n",
    "                        pred = id_pred[uid]\n",
    "                        name_pred[u] = pred\n",
    "                    else:\n",
    "                        # print 'user %d not in network predictions.'\n",
    "                        user_not_in_network += 1\n",
    "                        if text_prior == 'backoff':\n",
    "                            text_predition = str(text_preds[int(uid) - len(trainUsersLower)])\n",
    "                            pred = text_predition\n",
    "                        else:\n",
    "                            # pred = str(len(categories) / 2)  \n",
    "                            pred = classIndx\n",
    "                        name_pred[u] = pred\n",
    "\n",
    "                    preds.append(int(pred))\n",
    "                \n",
    "                print \"doubles found is \" + str(doubles_found)\n",
    "                print \"users with dummy labels: \" + str(dummy_count)\n",
    "                print \"users not in network: \" + str(user_not_in_network)\n",
    "                print \"total number of users: \" + str(len(U_eval))\n",
    "                # print preds\n",
    "                # print [int(i) for i in Y_test.tolist()]    \n",
    "                mad_mean, mad_median, mad_acc = loss(preds, U_eval, save_results_bucket_size=True, results_key=None)\n",
    "    return mad_mean, mad_median, mad_acc\n",
    "                \n",
    "                    \n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    ''' compute the weighted median of values list. The \n",
    "    weighted median is computed as follows:\n",
    "    1- sort both lists (values and weights) based on values.\n",
    "    2- select the 0.5 point from the weights and return the corresponding values as results\n",
    "    e.g. values = [1, 3, 0] and weights=[0.1, 0.3, 0.6] assuming weights are probabilities.\n",
    "    sorted values = [0, 1, 3] and corresponding sorted weights = [0.6, 0.1, 0.3] the 0.5 point on\n",
    "    weight corresponds to the first item which is 0. so the weighted median is 0.'''\n",
    "    \n",
    "    # convert the weights into probabilities\n",
    "    sum_weights = sum(weights)\n",
    "    weights = np.array([(w * 1.0) / sum_weights for w in weights])\n",
    "    # sort values and weights based on values\n",
    "    values = np.array(values)\n",
    "    sorted_indices = np.argsort(values)\n",
    "    values_sorted = values[sorted_indices]\n",
    "    weights_sorted = weights[sorted_indices]\n",
    "    # select the median point\n",
    "    it = np.nditer(weights_sorted, flags=['f_index'])\n",
    "    accumulative_probability = 0\n",
    "    median_index = -1\n",
    "    while not it.finished:\n",
    "        accumulative_probability += it[0]\n",
    "        if accumulative_probability >= 0.5:\n",
    "            median_index = it.index\n",
    "            return values_sorted[median_index]\n",
    "        elif accumulative_probability == 0.5:\n",
    "            median_index = it.index\n",
    "            it.iternext()\n",
    "            next_median_index = it.index\n",
    "            return mean(values_sorted[[median_index, next_median_index]])\n",
    "        it.iternext()\n",
    "\n",
    "    return values_sorted[median_index]\n",
    "\n",
    "\n",
    "\n",
    "#Here we can select the parameter for the models, GeoLoc Text -> text_classification\n",
    "# network_lp_regression -> GeoLoc LP\n",
    "# network_lp_regression with prior ==\"text\" -> GeoLoc Hybrid ,keep in mind that for hybrid the predictions for\n",
    "# GeoLoc Text have to be available under\n",
    "initialize(partitionMethod=partitionMethod, granularity=BUCKET_SIZE, write=False, readText=True, reload_init=False, regression=do_not_discretize)\n",
    "if 'text_classification' in models_to_run:\n",
    "    t_mean, t_median, t_acc, d_mean, d_median, d_acc = asclassification(granularity=BUCKET_SIZE, partitionMethod=partitionMethod, use_mention_dictionary=False, binary=binary, sublinear=sublinear, penalty=penalty, fit_intercept=fit_intercept, norm=norm, use_idf=use_idf)\n",
    "if 'network_lp_regression_collapsed' in models_to_run:\n",
    "    LP_collapsed(weighted=False, prior='none', normalize_edge=False, remove_celebrities=True, dev=False, project_to_main_users=True, node_order='random', remove_mentions_with_degree_one=False)\n",
    "if 'network_lp_regression' in models_to_run:\n",
    "    LP(weighted=True, prior='none', normalize_edge=True, remove_celebrities=False, dev=False, node_order='random')\n",
    "if 'network_lp_classification' in models_to_run:\n",
    "    LP_classification(weighted=True, prior='dongle', normalize_edge=False, remove_celebrities=True, dev=False, project_to_main_users=True, node_order='random', remove_mentions_with_degree_one=True)\n",
    "\n",
    "\n",
    "#junto_postprocessing(multiple=False, dev=False, method=\"partitionMethod\", celeb_threshold=15, weighted=True, text_prior=\"backoff\")\n",
    "\n",
    "print str(datetime.now())\n",
    "script_end_time = time.time()\n",
    "script_execution_hour = (script_end_time - script_start_time) / 3600.0\n",
    "print \"The script execution time (in hours) is \" + str(script_execution_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of GeoLoc Text is a .pkl file. We will read this .pkl file and write the predictions into a new CSV\n",
    "\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import csv \n",
    "\n",
    "# Path to prediction file of GeoLoc Text\n",
    "pathPKL=\"C:/Users/dennis/Desktop/geoLocation/results/results.pkl\"\n",
    "\n",
    "with open(preprocessedData, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Path to new CSV file which contains predictions of GeoLoc Text\n",
    "pathCSV=\"C:/Users/dennis/Desktop/dataTestTrain/results/\"+model+\".csv\"\n",
    "\n",
    "with open(newFile, 'wb') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    for x,y in data.items():\n",
    "        y=str(y)\n",
    "        y = y.replace(\"(\",\"\")\n",
    "        y=y.replace (\")\",\"\")\n",
    "        writer.writerow([x,y])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output for GeoLoc LP and GeoLoc Hybrid is in variable \"LPpredictions\". Therefore we will use this dictionary\n",
    "# to write the outputs from this variable into a CSV.\n",
    "\n",
    "import csv\n",
    "\n",
    "newFile = 'C:/Users/Dennis/Desktop/geoLocLP.csv'\n",
    "with open(newFile, 'wb') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    for key, value in LPpredictions.items():\n",
    "        new =[key,value]\n",
    "        writer.writerow(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next cells will take the predictions for the users and represent them on a tweet-level for the hydrated \n",
    "# TweetsCOV19 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country Codes\n",
    "ISO3166 = {\n",
    "\t'AD': 'Andorra',\n",
    "\t'AE': 'United Arab Emirates',\n",
    "\t'AF': 'Afghanistan',\n",
    "\t'AG': 'Antigua & Barbuda',\n",
    "\t'AI': 'Anguilla',\n",
    "\t'AL': 'Albania',\n",
    "\t'AM': 'Armenia',\n",
    "\t'AN': 'Netherlands Antilles',\n",
    "\t'AO': 'Angola',\n",
    "\t'AQ': 'Antarctica',\n",
    "\t'AR': 'Argentina',\n",
    "\t'AS': 'American Samoa',\n",
    "\t'AT': 'Austria',\n",
    "\t'AU': 'Australia',\n",
    "\t'AW': 'Aruba',\n",
    "\t'AZ': 'Azerbaijan',\n",
    "\t'BA': 'Bosnia and Herzegovina',\n",
    "\t'BB': 'Barbados',\n",
    "\t'BD': 'Bangladesh',\n",
    "\t'BE': 'Belgium',\n",
    "\t'BF': 'Burkina Faso',\n",
    "\t'BG': 'Bulgaria',\n",
    "\t'BH': 'Bahrain',\n",
    "\t'BI': 'Burundi',\n",
    "\t'BJ': 'Benin',\n",
    "\t'AFRICA/PORTO':'Benin',\n",
    "\t'BM': 'Bermuda',\n",
    "\t'BN': 'Brunei Darussalam',\n",
    "\t'BO': 'Bolivia',\n",
    "    'BQ':\"Bonaire, Sint Eustatius and Saba\",\n",
    "\t'BR': 'Brazil',\n",
    "\t'BS': 'Bahama',\n",
    "\t'BT': 'Bhutan',\n",
    "\t'BU': 'Burma (no longer exists)',\n",
    "\t'BV': 'Bouvet Island',\n",
    "\t'BW': 'Botswana',\n",
    "\t'BY': 'Belarus',\n",
    "\t'BZ': 'Belize',\n",
    "\t'CA': 'Canada',\n",
    "\t'CC': 'Cocos (Keeling) Islands',\n",
    "\t'CF': 'Central African Republic',\n",
    "\t'CD': 'Congo',\n",
    "\t'CG': 'Congo',\n",
    "\t'CH': 'Switzerland',\n",
    "\t'CI': 'Côte D\\'ivoire (Ivory Coast)',\n",
    "\t'CK': 'Cook Iislands',\n",
    "\t'CL': 'Chile',\n",
    "\t'CM': 'Cameroon',\n",
    "\t'CN': 'China',\n",
    "\t'CO': 'Colombia',\n",
    "\t'CR': 'Costa Rica',\n",
    "\t'CS': 'Czechoslovakia (no longer exists)',\n",
    "\t'CU': 'Cuba',\n",
    "\t'CV': 'Cape Verde',\n",
    "\t'CX': 'Christmas Island',\n",
    "    'CW': 'Netherlands Antilles',\n",
    "\t'CY': 'Cyprus',\n",
    "\t'CZ': 'Czech Republic',\n",
    "\t'DD': 'German Democratic Republic (no longer exists)',\n",
    "\t'DE': 'Germany',\n",
    "\t'DJ': 'Djibouti',\n",
    "\t'DK': 'Denmark',\n",
    "\t'DM': 'Dominica',\n",
    "\t'DO': 'Dominican Republic',\n",
    "\t'DZ': 'Algeria',\n",
    "\t'EC': 'Ecuador',\n",
    "\t'EE': 'Estonia',\n",
    "\t'EG': 'Egypt',\n",
    "\t'EH': 'Western Sahara',\n",
    "\t'ER': 'Eritrea',\n",
    "\t'ES': 'Spain',\n",
    "\t'ET': 'Ethiopia',\n",
    "\t'FI': 'Finland',\n",
    "\t'FJ': 'Fiji',\n",
    "\t'FK': 'Falkland Islands (Malvinas)',\n",
    "\t'FM': 'Micronesia',\n",
    "\t'FO': 'Faroe Islands',\n",
    "\t'FR': 'France',\n",
    "\t'FX': 'France, Metropolitan',\n",
    "\t'GA': 'Gabon',\n",
    "\t'GB': 'United Kingdom',\n",
    "\t'GD': 'Grenada',\n",
    "\t'GE': 'Georgia',\n",
    "\t'GF': 'French Guiana',\n",
    "    'GG': 'Guernsey',\n",
    "\t'GH': 'Ghana',\n",
    "\t'GI': 'Gibraltar',\n",
    "\t'GL': 'Greenland',\n",
    "\t'GM': 'Gambia',\n",
    "\t'GN': 'Guinea',\n",
    "\t'GP': 'Guadeloupe',\n",
    "\t'GQ': 'Equatorial Guinea',\n",
    "\t'GR': 'Greece',\n",
    "\t'GS': 'South Georgia and the South Sandwich Islands',\n",
    "\t'GT': 'Guatemala',\n",
    "\t'GU': 'Guam',\n",
    "\t'GW': 'Guinea-Bissau',\n",
    "\t'GY': 'Guyana',\n",
    "\t'HK': 'Hong Kong',\n",
    "\t'HM': 'Heard & McDonald Islands',\n",
    "\t'HN': 'Honduras',\n",
    "\t'HR': 'Croatia',\n",
    "\t'HT': 'Haiti',\n",
    "\t'HU': 'Hungary',\n",
    "\t'ID': 'Indonesia',\n",
    "\t'IE': 'Ireland',\n",
    "\t'IL': 'Israel',\n",
    "\t'IN': 'India',\n",
    "    'IM': 'Isle of Man',\n",
    "\t'IO': 'British Indian Ocean Territory',\n",
    "\t'IQ': 'Iraq',\n",
    "\t'IR': 'Islamic Republic of Iran',\n",
    "\t'IS': 'Iceland',\n",
    "\t'IT': 'Italy',\n",
    "    'JE': \"Jersey\",\n",
    "\t'JM': 'Jamaica',\n",
    "\t'JO': 'Jordan',\n",
    "\t'JP': 'Japan',\n",
    "\t'KE': 'Kenya',\n",
    "\t'KG': 'Kyrgyzstan',\n",
    "\t'KH': 'Cambodia',\n",
    "\t'KI': 'Kiribati',\n",
    "\t'KM': 'Comoros',\n",
    "\t'KN': 'St. Kitts and Nevis',\n",
    "\t'KP': 'Korea, Democratic People\\'s Republic of',\n",
    "\t'KR': 'Korea, Republic of',\n",
    "\t'KW': 'Kuwait',\n",
    "\t'KY': 'Cayman Islands',\n",
    "\t'KZ': 'Kazakhstan',\n",
    "\t'LA': 'Lao People\\'s Democratic Republic',\n",
    "\t'LB': 'Lebanon',\n",
    "\t'LC': 'Saint Lucia',\n",
    "\t'LI': 'Liechtenstein',\n",
    "\t'LK': 'Sri Lanka',\n",
    "\t'LR': 'Liberia',\n",
    "\t'LS': 'Lesotho',\n",
    "\t'LT': 'Lithuania',\n",
    "\t'LU': 'Luxembourg',\n",
    "\t'LV': 'Latvia',\n",
    "\t'LY': 'Libyan Arab Jamahiriya',\n",
    "\t'MA': 'Morocco',\n",
    "\t'MC': 'Monaco',\n",
    "\t'MD': 'Moldova, Republic of',\n",
    "    'ME': 'Montenegro',\n",
    "\t'MG': 'Madagascar',\n",
    "\t'MH': 'Marshall Islands',\n",
    "\t'MK': 'North Macedonia',\n",
    "\t'ML': 'Mali',\n",
    "\t'MN': 'Mongolia',\n",
    "\t'MM': 'Myanmar',\n",
    "\t'MO': 'Macau',\n",
    "\t'MP': 'Northern Mariana Islands',\n",
    "\t'MQ': 'Martinique',\n",
    "\t'MR': 'Mauritania',\n",
    "\t'MS': 'Monserrat',\n",
    "\t'MT': 'Malta',\n",
    "\t'MU': 'Mauritius',\n",
    "\t'MV': 'Maldives',\n",
    "\t'MW': 'Malawi',\n",
    "\t'MX': 'Mexico',\n",
    "\t'MY': 'Malaysia',\n",
    "\t'MZ': 'Mozambique',\n",
    "\t'NA': 'Namibia',\n",
    "\t'NC': 'New Caledonia',\n",
    "\t'NE': 'Niger',\n",
    "\t'NF': 'Norfolk Island',\n",
    "\t'NG': 'Nigeria',\n",
    "\t'NI': 'Nicaragua',\n",
    "\t'NL': 'Netherlands',\n",
    "\t'NO': 'Norway',\n",
    "\t'NP': 'Nepal',\n",
    "\t'NR': 'Nauru',\n",
    "\t'NT': 'Neutral Zone (no longer exists)',\n",
    "\t'NU': 'Niue',\n",
    "\t'NZ': 'New Zealand',\n",
    "\t'OM': 'Oman',\n",
    "\t'PA': 'Panama',\n",
    "\t'PE': 'Peru',\n",
    "\t'PF': 'French Polynesia',\n",
    "\t'PG': 'Papua New Guinea',\n",
    "\t'PH': 'Philippines',\n",
    "\t'PK': 'Pakistan',\n",
    "\t'PL': 'Poland',\n",
    "    'PS': 'Palestine',\n",
    "\t'PM': 'St. Pierre & Miquelon',\n",
    "\t'PN': 'Pitcairn',\n",
    "\t'PR': 'Puerto Rico',\n",
    "\t'PT': 'Portugal',\n",
    "\t'PW': 'Palau',\n",
    "\t'PY': 'Paraguay',\n",
    "\t'QA': 'Qatar',\n",
    "\t'RE': 'Réunion',\n",
    "\t'RO': 'Romania',\n",
    "\t'RU': 'Russian Federation',\n",
    "\t'RS': 'Serbia',\n",
    "\t'RW': 'Rwanda',\n",
    "\t'SA': 'Saudi Arabia',\n",
    "\t'SB': 'Solomon Islands',\n",
    "\t'SC': 'Seychelles',\n",
    "\t'SD': 'Sudan',\n",
    "\t'SE': 'Sweden',\n",
    "\t'SG': 'Singapore',\n",
    "\t'SH': 'St. Helena',\n",
    "\t'SI': 'Slovenia',\n",
    "\t'SJ': 'Svalbard & Jan Mayen Islands',\n",
    "\t'SK': 'Slovakia',\n",
    "\t'SL': 'Sierra Leone',\n",
    "\t'SM': 'San Marino',\n",
    "\t'SN': 'Senegal',\n",
    "\t'SO': 'Somalia',\n",
    "\t'SR': 'Suriname',\n",
    "\t'ST': 'Sao Tome & Principe',\n",
    "    'SS': 'South Sudan',\n",
    "\t'SU': 'Union of Soviet Socialist Republics (no longer exists)',\n",
    "\t'SV': 'El Salvador',\n",
    "\t'SY': 'Syrian Arab Republic',\n",
    "\t'SZ': 'Swaziland',\n",
    "\t'TC': 'Turks & Caicos Islands',\n",
    "\t'TD': 'Chad',\n",
    "\t'TF': 'French Southern Territories',\n",
    "\t'TG': 'Togo',\n",
    "\t'TH': 'Thailand',\n",
    "\t'TJ': 'Tajikistan',\n",
    "\t'TK': 'Tokelau',\n",
    "    'TL': 'Timor Leste',\n",
    "\t'TM': 'Turkmenistan',\n",
    "\t'TN': 'Tunisia',\n",
    "\t'TO': 'Tonga',\n",
    "\t'TP': 'East Timor',\n",
    "\t'TR': 'Turkey',\n",
    "\t'TT': 'Trinidad & Tobago',\n",
    "\t'TV': 'Tuvalu',\n",
    "\t'TW': 'Taiwan, Province of China',\n",
    "\t'TZ': 'Tanzania, United Republic of',\n",
    "\t'UA': 'Ukraine',\n",
    "\t'UG': 'Uganda',\n",
    "\t'UM': 'United States Minor Outlying Islands',\n",
    "\t'US': 'United States',\n",
    "\t'UY': 'Uruguay',\n",
    "\t'UZ': 'Uzbekistan',\n",
    "\t'VA': 'Vatican City State (Holy See)',\n",
    "\t'VC': 'St. Vincent & the Grenadines',\n",
    "\t'VE': 'Venezuela',\n",
    "\t'VG': 'British Virgin Islands',\n",
    "\t'VI': 'United States Virgin Islands',\n",
    "\t'VN': 'Viet Nam',\n",
    "\t'VU': 'Vanuatu',\n",
    "\t'WF': 'Wallis & Futuna Islands',\n",
    "\t'WS': 'Samoa',\n",
    "\t'YD': 'Democratic Yemen (no longer exists)',\n",
    "\t'YE': 'Yemen',\n",
    "\t'YT': 'Mayotte',\n",
    "\t'YU': 'Yugoslavia',\n",
    "\t'ZA': 'South Africa',\n",
    "    'XK': 'Kosovo',\n",
    "\t'ZM': 'Zambia',\n",
    "\t'ZR': 'Zaire',\n",
    "\t'ZW': 'Zimbabwe',\n",
    "\t'ZZ': 'Unknown or unspecified country',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output for the home location of a user is in form of geocoordinates. For geo-tagging\n",
    "# TweetsCOV19 we will need location information. Therefore we will first reverse geocode\n",
    "# each prediction with \"reverse_geocoder\"\n",
    "\n",
    "Reverse Geocoding for all user and export to csv\n",
    "import csv\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "pathResultsCoord=\"C:/Users/dennis/Desktop/geoLocLP.csv\"\n",
    "pathResultsLocation = \"C:/Users/dennis/Desktop/geoLocLPLocation.csv\"\n",
    "\n",
    "dataUser=[]\n",
    "dataCoord=[]\n",
    "\n",
    "with open(pathResultsLocation, 'a',newline='') as CSV:\n",
    "    writer = csv.writer(CSV, delimiter=';')\n",
    "    \n",
    "    with open(pathResultsCoord, 'r')as old:\n",
    "        reader = csv.reader(old, delimiter=';', quotechar='|')\n",
    "        #Reading CSV\n",
    "        for line in reader:\n",
    "            userID=line[0]\n",
    "            #Converting coordinates into tuple\n",
    "            \n",
    "            coordinates = line[1].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "            coord1 =coordinates[0]\n",
    "            coord2 =coordinates[1]\n",
    "            coordFinal = (coord1,coord2)\n",
    "            \n",
    "            dataUser.append(userID)\n",
    "            dataCoord.append(coordFinal)\n",
    "            \n",
    "        #Reverse geocoding\n",
    "        results = rg.search(dataCoord)\n",
    "\n",
    "        for x in range(len(dataUser)):\n",
    "                user = dataUser[x]\n",
    "                location=results[x]\n",
    "                #if location[\"cc\"]==\"IM\":\n",
    "                #    print(location[\"name\"])\n",
    "                 #   print(location[\"admin1\"])\n",
    "                #    print(location[\"lat\"],location[\"lon\"])\n",
    "                country = ISO3166[location[\"cc\"]]\n",
    "                county = location[\"admin1\"]\n",
    "                city = location[\"name\"]\n",
    "                \n",
    "                # For UK we will need county information\n",
    "                if country ==\"United Kingdom\":\n",
    "                    county = location[\"admin2\"]\n",
    "                \n",
    "                newLine=[user,country,county,city]\n",
    "                writer.writerow(newLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will Create a Dict from the hydrated TweetsCOV19 as follows-> tweetID : userID\n",
    "# and then we will merge both files and create the final CSV to geo-tagg TweetsCOV19\n",
    "import json,csv\n",
    "\n",
    "tweetJSON=\"C:/Users/dennis/Desktop/BachelorArbeit/HydratedALL.jsonl\"\n",
    "\n",
    "dictTweet={}\n",
    "with open(tweetJSON,encoding=\"utf8\",errors='ignore') as allTweets:\n",
    "    for raw in allTweets: \n",
    "        tweet = json.loads(raw)\n",
    "        tweetID=tweet[\"id_str\"]\n",
    "        \n",
    "        userID=tweet[\"user\"][\"id_str\"]\n",
    "        #alternatively you can use the username instead of the user id\n",
    "        #userID=tweet[\"user\"][\"screen_name\"].lower()\n",
    "        line={}\n",
    "        dictTweet[tweetID]=userID\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pathPredictions = \"C:/Users/dennis/Desktop/geoLocLP2.csv\"\n",
    "\n",
    "df_geo = pd.read_csv(pathPredictions, delimiter =';',names=['user_id','country',\"county\",\"city\"], encoding =\"latin1\")\n",
    "df_dict = pd.DataFrame(list(dictTweet.items()),columns=[\"tweet_id\",\"user_id\"])\n",
    "\n",
    "#print(df_dict.head())\n",
    "#df_merged=pd.merge(df_geo,df_dict, left_on='user_id', right_on='user_id', how='left')\n",
    "\n",
    "#We will merge both files to create a final CSV containing the tweetID for the predictions\n",
    "df_merged=pd.merge(df_geo,df_dict.astype(\"int64\"), left_on='user_id', right_on='user_id', how='left')\n",
    "\n",
    "pathPredictionsFINAL = \"C:/Users/dennis/Desktop/geoLocLPFinal.csv\"\n",
    "header = [\"tweet_id\",\"country\",\"county\",\"city\"]\n",
    "\n",
    "df_merged.to_csv(pathPredictionsFINAL, columns = header,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
